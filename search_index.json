[
["index.html", "Transparent Statistics Guidelines Preface Organization Using Limitations Versions Citing Contributing", " Transparent Statistics Guidelines Transparent Statistics in HCI Working Group (http://transparentstatistics.org/) 2019-06-29 Preface This document grew out of a Special Interest Group and Workshop to develop guidelines for transparent statistical communication in Human-Computer Interaction research. The recommendations in this guideline are chosen to encourage transparency in the practice of statistics. For more details on this position, see our guiding principles. Organization This document is organized according to statistical topics. Each chapter splits into two sections: FAQ and Exemplars. The FAQ section lists questions that are relevant to transparency practice. We strive to allow random-access, to allow each reader to quickly find a compact answers to specific questions he/she has in mind. Thus, this guideline assumes some knowledge about the topic. We summarize such knowledge in earlier questions, but this is by no means exhaustive. Whenever you found yourself disoriented, it might be helpful to skim one or two earlier questions or lookup definitions of statistical terms over the internet. Whenever appropriate, we also point to other questions and parts of exemplars that are relevant. The Exemplars section supplements the FAQ by showing concrete cases in code and their interpretation(s). In contrast to the FAQ, the exemplars should be read top-to-bottom: the earlier examples are building blocks of the latter. Using The text in this guideline is under CC-BY license, and the code is under MIT license. These license choices mean that you can use any code from this guideline in your analysis and can release your code (even under other licenses). Limitations We focus on addressing issues that are commonly misunderstood and suboptimally practiced—intentionally or unintentionally. Therefore, the scope and focus of this document is far narrower than being a complete textbook in statistics. Versions As the consensus in statistical practice evolves (as science evolves), this document is a living document. For the most up-to-date version of the guideline, visit the online guideline page or source on Github. Citing You can cite this guideline with the following ACM reference format. If you wish to cite a specific version, replace the DOI with the version-specific DOI and the date of publication as stated at the top of this webpage. Transparent Statistics in Human–Computer Interaction working group. 2019. Transparent Statistics Guidelines. (Jun 2019). DOI: http://dx.doi.org/10.5281/zenodo.1186169 (Available at https://transparentstats.github.io/guidelines) Here’s the same thing in BibTeX format @misc{TransparentStatsJun2019, title={{Transparent} {Statistics} {Guidelines}}, author={{Transparent} {Statistics} in {Human}–{Computer} {Interaction} {Working} {Group}}, DOI = {10.5281/zenodo.1186169}, month={Jun}, year={2019}, note={({Available} at https://transparentstats.github.io/guidelines)} } Contributing We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. Especially, please have a look at the Style Guide and a contributor Code of Conduct. "],
["principles.html", "Chapter 1 Transparent Statistics Guiding Principles 1.1 Introduction 1.2 Guiding Principles", " Chapter 1 Transparent Statistics Guiding Principles Version: 1.0 Contributed to the writing: Pierre Dragicevic, Chat Wacharamanotham, Matthew Kay Gave feedback: Endorsed: 1.1 Introduction Human-computer interaction (HCI) is a large, multidisciplinary field drawing on a variety of approaches for analyzing quantitative data. However, many of our existing practices have drawn increasing criticism, such as our overreliance on mechanical statistical testing procedures, our lack of replications and meta-analyses, and our unwillingness to share data and study materials. These issues have been discussed within HCI (Wilson et al. 2011; Kaptein and Robertson 2012; Dragicevic 2016; Kay, Nelson, and Hekler 2016; Cockburn, Gutwin, and Dix 2018) and (to a much larger extent) in other fields (Cohen 1994; Gigerenzer 2004; Ioannidis 2005; Simmons, Nelson, and Simonsohn 2011; Giner-Sorolla 2012; Cumming 2014; Nosek et al. 2017). Poor statistical practice and the lack of transparency in statistical reporting hamper the progress of knowledge and undermine the scientific credibility of affected disciplines, as witnessed by the growing number of press articles reporting a “crisis of confidence” in the most visible of these disciplines (Earp and Trafimow 2015). The purpose of the transparent statistics guidelines is not to admonish an entire field of researchers for their existing practices nor to urge them to adopt a specific set of methods. There is no universal inference procedure that can act as a substitute for good statistical reasoning (Gigerenzer and Marewski 2015). The multifaceted nature of HCI also means we need to embrace a variety of practices. A fixed set of DOs and DON’Ts would be both too brittle to change over time and too restrictive in the face of the various ways of generating knowledge in our field. Instead, we propose to advance a general vision of transparent statistics that HCI researchers can draw inspiration from, and that is largely method-agnostic. We refer to transparent statistics simply as “a philosophy of statistical reporting whose purpose is to advance scientific knowledge rather than to persuade”. Regardless of the methods used, we aim to provide guidance that makes the communication of those methods more transparent, that makes reproduction and replication of work easier, and that makes evaluation of work (e.g., by peer reviewers) easier and more fair. To that end, a “transparent statistics” initiative was started in 2016, whose purpose is to discuss ways of promoting transparent statistics at CHI and suggest a series of incremental changes within the community (“Transparent Statistics Website” 2017). These include more specific author and reviewer guidelines, exemplars for authors, and “badges” (“Open Science Badges” 2017). The goal of the initiative is to address questions such as: what can an author do to improve the transparency of their communication? What can a reviewer do to encourage and reward transparency? What changes to the review process might encourage transparency and incentivize researchers? In this way we hope to avoid the time-honored tradition of admonishing researchers for doing statistics poorly, and instead encourage them—and guide them—to do better. The goal of this first chapter is to lay out the high-level principles on which other chapters will be based. Like other chapters, this chapter is not meant to be fixed in stone, but is meant to be constantly evolving and iteratively refined by the CHI community. 1.2 Guiding Principles Again, transparent statistics is “a philosophy of statistical reporting whose purpose is to advance scientific knowledge rather than to persuade”. This idea is not new. For example, the following quote from Ronald Fisher captures the essence of transparent statistics: “we have the duty of […] communicating our conclusions in intelligible form, in recognition of the right of other free minds to utilize them in making their own decisions.” (Fisher 1955). More recent writings have emphasized the importance of contributing useful and accurate knowledge over telling compelling and convincing stories (Giner-Sorolla 2012; Dragicevic 2016). Based on these visions, we propose a set of nine guiding principles for writing transparent statistical reports: 1) faithfulness, 2) robustness, 3) resilience, 4) process transparency, 5) clarity, 6) simplicity, 7) non-contingency, 8) precision and economy, and 9) material availability. 1. Faithfulness At the most basic level, a transparent statistical report should strive to be faithful to the data and the phenomena studied. This means that it should strive to capture and convey the “truth” as accurately as possible, especially concerning the uncertainty within the data. Major sources of uncertainty need to be carefully assessed and propagated to the presentations and interpretations of the results, all the way up to the final conclusions. Conclusions should be nuanced and stress the uncertainty in the data and in the process. Example: It is evident that any major error in an analysis will result in findings that are likely not faithful to the data and the phenomena studied. This includes effect estimates that are very different from the true effect, but also measures of uncertainty that fail to capture the true uncertainty. Example: Exaggerating findings by presenting uncertain results as certain is unfaithful to the data. Example: A study report that analyzes all its data carefully but fails to acknowledge important issues with data validity (e.g., non-random condition assignment) is faithful to the data but unfaithful to the phenomena studied. The same goes with over-generalizing findings. 2. Robustness In order to minimize the likelihood of inaccurate (unfaithful) results, data analysis and reporting strategies that are robust to departures from statistical assumptions – or that make few assumptions – should ideally be preferred. Given that statistical assumptions are never met perfectly, the question should not be “are the assumptions met?” but instead “what are the likely consequences of such and such departure?”. Thus, it is hugely beneficial for researchers to know how their methods behave depending on the nature and degree of possible departures, so that they can explain it in their report when necessary. When uncertain, methods that are known for their robustness are safer choices. Example: ANOVA is robust to the normality assumption, and can in some cases give accurate results with unusual distributions and very small sample sizes (Norman 2010). Example: Bootstrapping makes no assumption about data distribution and is robust to departures from its own statistical assumptions, even though these assumptions are implausible (Kirby and Gerlanc 2013). 3. Resilience Data analysis and reporting strategies should be resilient to statistical noise, i.e., they should yield similar outcomes across hypothetical replications of the same study. Researchers should ask themselves how their statistical report would change if they took another random sample from the same population, and should try to make claims that are as robust as possible to these changes. In practice, the principle of resilience implies that researchers should avoid presenting statistical noise as signal, either by overfitting, or by overinterpreting patterns in results. It also implies that study reports should be smooth functions of the data. This means that data analysis and presentation strategies should be chosen so that similar experimental datasets yield similar results, interpretations and conclusions (Dixon 2003; Dragicevic 2016; “Statistical Dances: Why No Statistical Analysis Is Reliable and What to Do About It” 2017). The principle of resilience is important and is directly relevant to the issue of study replicability. Example: Presenting a bar chart of means without error bars and commenting on the emerging patterns is akin to overfitting and is thus not resilient. Example: Computing and reporting 95% interval estimates is resilient, but drawing binary conclusions based on whether they contain zero is not, because two very similar datasets may yield seemingly very different scientific conclusions (Cumming 2013). Example: For the same reasons, computing Bayes factors and interpreting them strictly based on conventional thresholds violates the principle of resilience (“Dance of the Bayes Factors” 2016). 4. Process Transparency A core aspect of transparent statistics is that data analysis and reporting strategies need to be explained rather than implied. The decisions made during the analysis and report writing should be communicated as explicitly as possible, as the results of an analysis cannot be fairly assessed and understood if many decisions remain concealed (Giner-Sorolla 2012; Gelman and Loken 2013). At the most basic level, researchers should ideally state which portions of their data analysis were planned before the data was seen, and which portions were not. Analyses that are fully planned can be referred to as prespecified, while analyses that are largely unplanned should be referred to as exploratory (Cumming 2014, 10). Both types of analysis are valid, although the former allows to support stronger claims than the latter. Process transparency also implies faithfully reporting what were the research goals, the research questions, and (optionally) the researcher’s expectations prior to seeing the data (Kerr 1998; Gelman and Loken 2013; Cockburn, Gutwin, and Dix 2018). Results from analyses need to be reported whether or not they meet the researcher’s initial expectations. When this is not the case, the rationale for selecting results needs be explained. Finally, sharing data and analysis scripts greatly benefits process transparency. Example: Hypothesizing after the results are known (or “HARKing”) strongly goes against process transparency (Kerr 1998). Researchers who do not have clear expectations should state research questions instead of hypotheses (Cumming 2013). Example: Cherry-picking “convenient” results (e.g., results that best support the hypotheses), or trying multiple alternative analyses and reporting only those that are convenient clearly violates process transparency (Simmons, Nelson, and Simonsohn 2011). Example: Even when a researcher has no preference for a given hypothesis and no intention to p-hack, cherry-picking results to give the impression of a coherent story also goes against process transparency (Giner-Sorolla 2012; Gelman and Loken 2013). Example: Provided an analysis is presented as exploratory, trying multiple analyses and reporting the most interesting and informative results by taking a neutral stance is perfectly acceptable and does not go against process transparency (Tukey 1977). Transparency can however be increased by explaining what has been tried. 5. Clarity Study reports should be as easy to understand as possible, because as explained by Ronald Fisher (quoted above), readers and reviewers cannot judge an analysis without understanding. There are two facets of clarity: ease of processing and accessibility. Study reports should be easy to process, even when they target experts. When results can be communicated more effectively with visual representations than with numerals, visual representations should be preferred (Loftus 1993; Gelman, Pasarica, and Dodhia 2002). Although a study report should communicate as much relevant information as possible, information overload must be avoided by reporting non-essential information in appendices or in supplemental material. Second, study reports should ideally be accessible to most members of the HCI community, instead of being comprehensible by only a handful of specialists. The more accessible an analysis is, the more the “free minds” who can judge it. Thus a study report should be more an exercise of pedagogy than an exercise of rhetoric. The goal of a statistical report is not to signal expertise, but to explain. Example: Using statistics for defensive purposes by generating p-cluttered reports rather than informative plots violates the principle of clarity. Example: Excessive numbers of significant digits are difficult to process thus they go against the principle of clarity (Ehrenberg 1977), in addition to giving a misleading impression of precision (Taylor 1997). 6. Simplicity When choosing between two data analysis procedures, the simplest procedure should ideally be preferred even if it is slightly inferior in other respects. A focus on simplicity follows from the principles of clarity and ease of processing, and it makes both researcher mistakes and reader misinterpretations less likely to occur. In other words, the KISS principle (Keep It Simple, Stupid) is as relevant in transparent statistics as in any other domain. 7. Non-contingency When possible and outside exploratory analyses, data analysis and reporting strategies should avoid decisions that are contingent on data, e.g., “if the data turns out like this, compute this, or report that”. This principle follows from the principles of process transparency, clarity, and simplicity, because data-contingent procedures are hard to explain and easy to leave unexplained (Gelman and Loken 2013). It is also a corollary of the principle of resilience because any dichotomous decision decreases a procedure’s resilience to statistical noise. Carefully planning an analysis is a good way to make sure that the principle of non-contingency is met (Cumming 2014), especially if all the analysis code has been written ahead of time based on pilot data (Dragicevic 2016). Pre-registering an analysis further increases transparency by allowing anyone to verify that the plan has been followed (Nosek et al. 2017; Cockburn, Gutwin, and Dix 2018). In exploratory analyses and in complex modeling problems, which are often data-contingent by nature, the principle of non-contingency should be applied to the best effort. Example: Using a test of normality to decide whether to use parametric or non-parametric methods violates the principle of non-contingency, in addition to not being very useful (Stewart-Oaten 1995; Wierdsma 2013). If the test of normality is not mentioned in the report, it additionally violates the principle of process transparency. Example: Selective reporting of data (i.e., cherry-picking) clearly violates the non-contingency principle, and generally also the principles of faithfulness and of process transparency. It is only acceptable if the analysis is clearly presented as exploratory, and if the goal of the selection is to learn from the data rather than to support a convenient hypothesis or story. 8. Precision and economy Data quality (Gelman 2017) and high statistical power (Cohen 1994), which in the estimation world translates to high statistical precision (Cumming 2013; Kruschke and Liddell 2017), are important goals to pursue. This is because even if full transparency is achieved, a study report where nothing conclusive can be said would be a waste of readers’ time, and may prompt them to seek inexistent patterns. Precision depends on experiment design, but also on the choice of analysis methods – thus analysis methods that yield high precision should be prefered. However, researchers should strive to avoid false precision, e.g., reporting numerical results without information about their uncertainty and/or with way more significant digits than justified by their uncertainty (Taylor 1997). Analysis and reporting strategies that waste statistical power and precision (e.g., by dichotomizing continuous variables) should also be ideally avoided (Dragicevic 2016). Though the economy principle is not directly related to transparency, it is generally advisable not to waste data. It is a sensible goal for researchers to try to learn as much as possible from a study, provided that the principles of faithfulness and process transparency are carefully kept in mind. For similar reasons, while it is essential that researchers do not read too much in their data and do not fall for confirmation bias, exploratory analyses are often very informative and should thus be encouraged. The best study reports combine prespecified with exploratory analyses, while clearly distinguishing between the two. 9. Material availability Sharing as much study material as possible is a core part of transparent statistics, as it greatly facilitates peer scrutiny and replication. Being able to run the experimental software and examine what participants saw (the techniques, tasks, instructions, and questions asked) is essential in order for other researchers to understand the details of a study. In addition, sharing the source code of the experimental software greatly facilitates replication. Similarly, experimental data (all data files and if possible analysis scripts) is necessary for conducting re-analyses and meta-analyses. Although uploading supplementary material makes sense during the reviewing phase, to be really useful all material should be freely shared online upon paper acceptance, ideally on a website that can guarantee long-term accessibility. References "],
["effectsize.html", "Chapter 2 Effect size 2.1 FAQ 2.2 Exemplar: Simple effect size 2.3 Exemplar: Within-subjects experiment 2.4 Exemplar: Standardized effect size 2.5 Exemplar: Nonparametric effect size", " Chapter 2 Effect size Version: 1.1 Contributed to the writing: Matthew Kay, Chat Wacharamanotham, Steve Haroz, Pierre Dragicevic, Jacob O. Wobbrock, Janne Lindqvist, Yea-Seul Kim, Amelia McNamara Gave feedback: Fanny Chevalier, Petra Isenberg, Christian Cherek, Hendrik Heuer, Michael McGuffin, Krishna Subramanian, Philipp Wacker, Michael Correll, Jake Hofman, Lewis Chuang, Christian P. Janssen, Jeff Huang, Julie Schiller, Sayamindu Dasgupta, Benjamin Johansen, Gijs Huisman, Florian Echtler, Kai Lukoff, Jennifer Lee Carlson, Nediyana Daskalova, Jennifer J. McGrath, Emanuel Zgraggen Endorsed: (Pending) So far, this document covers the effect sizes of the difference between means. There are other types of effect sizes such as correlation as well. If you are interested to contribute this part, get in touch! 2.1 FAQ 2.1.1 What is an effect size? Broadly speaking, an effect size is “anything that might be of interest” (Cumming 2013); it is some quantity that captures the magnitude of the effect studied. In HCI, common examples of effect size include the mean difference (e.g., in seconds) in task completion times between two techniques (e.g., using a mouse vs. keyboard), or the mean difference in error rates (e.g., in percent). These are called simple effect sizes (or unstandardized effect sizes). More complex measures of effect size exist called standardized effect sizes (see What is a standardized effect size?). Although the term effect size is often used to refer to standardized effect sizes only, using the term in a broader sense can avoid unnecessary confusion (Cumming 2013; Wilkinson 1999a). In this document, “effect size” refers to both simple and standardized effect sizes. 2.1.2 Why and when should effect sizes be reported? In quantitative experiments, effect sizes are among the most elementary and essential summary statistics that can be reported. Identifying the effect size(s) of interest also allows the researcher to turn a vague research question into a precise, quantitative question (Cumming 2014). For example, if a researcher is interested in showing that their technique is faster than a baseline technique, an appropriate choice of effect size is the mean difference in completion times. The observed effect size will indicate not only the likely direction of the effect (e.g., whether the technique is faster or slower), but also whether the effect is large enough to care about. For the sake of transparency, effect sizes should always be reported in quantitative research, unless there are good reasons not to do so. According to the American Psychological Association: For the reader to appreciate the magnitude or importance of a study’s findings, it is almost always necessary to include some measure of effect size in the Results section. (American Psychological Association 2001) Sometimes, effect sizes can be hard to compute or to interpret. When this is the case, and if the main focus of the study is on the direction (rather than magnitude) of the effect, reporting the results of statistical significance tests without reporting effect sizes (see the inferential statistics FAQ) may be an acceptable option. 2.1.3 How should effect sizes be reported? The first choice is on whether to report simple effect sizes or standardized effect sizes. For this question, see Should simple effect sizes or standardized effect sizes be reported? It is rarely sufficient to report an effect size as a single quantity. This is because a single quantity like a difference in means or a Cohen’s d is typically only a point estimate, i.e., it is merely a best guess of the true effect size. It is crucial to also assess and report the statistical uncertainty about this point estimate. For more on assessing and reporting statistical uncertainty, see the inferential statistics FAQ. Ideally, an effect size report should include: The direction of the effect if applicable (e.g., given a difference between two treatments A and B, indicate if the measured effect is A - B or B - A). The type of point estimate reported (e.g., a sample mean difference) The type of uncertainty information reported (e.g., a 95% confidence interval, or a credible interval, or the standard deviation, or standard error) The units of the effect size if applicable (e.g., taps/minute or completion time in ms), or the type of standardized effect size if it is a unitless effect size. This information can be reported either numerically or graphically. Both formats are acceptable, although plots tend to be easier to comprehend than numbers when more than one effect size needs to be conveyed (Loftus 1993; Kastellec and Leoni 2007). Unless precise numerical values are important, it is sufficient (and often preferable) to report all effect sizes graphically. Researchers should avoid plotting point estimates without also plotting uncertainty information (using, e.g., error bars). ▸ Exemplar: simple effect size (specifically in the “Reporting simple effect size” sub-section) 2.1.4 What is a standardized effect size? A standardized effect size is a unitless measure of effect size. The most common measure of standardized effect size is Cohen’s d, where the mean difference is divided by the standard deviation of the pooled observations (Cohen 1988) \\(\\frac{\\text{mean difference}}{\\text{standard deviation}}\\). Other approaches to standardization exist [prefer citations]. To some extent, standardized effect sizes make it possible to compare different studies in terms of how “impressive” their results are (see How do I know my effect is large enough?); however, this practice is not without criticism (see the section Standardized mean differences let us compare and summarize results when studies use different outcome scales of (Cummings 2011)). 2.1.5 Should simple or standardized effect sizes be reported? This is a controversial issue 2.1.5.1 Standardized effect sizes Standardized effect sizes are useful when effects expressed in different units need to be combined or compared (Cumming 2014), e.g., a metaanalysis of a literature where results are reported using different units. However, even this practice is controversial, as it can rely on assumptions about the effects being measured that are difficult to verify (Cummings 2011). However, interpretations of standardized effect sizes should be accompanied by an argument for its applicability to the domain. If there is no inherent rationale for a particular interpretation of the practical significance of a standardized effect size, it should be accompanied by another assessment of the practical significance of the effect. Cohen’s rule of thumb for what constitutes a small, medium, or large effect size is specific to his domain and has been shown not to be generalizable. See controversial issues about effect sizes 2.1.5.2 Simple effect sizes Based on the principle of simplicity, simple effect sizes should be preferred over standardized effect sizes: Only rarely will uncorrected standardized effect size be more useful than simple effect size. It is usually far better to report simple effect size. (Baguley 2009) Simple effect sizes are often easier to interpret and justify (Cumming 2014; Cummings 2011). When the units of the data are meaningful (e.g., seconds), reporting effect sizes expressed in their original units is more informative and can make it easier to judge whether the effect has a practical significance (Wilkinson 1999a; Cummings 2011). Reporting simple effect sizes also allow future researchers to estimate and interpret variance with Bayesian methods. Reporting the simple effect size also consistent with the principle of simplicity ▸ Exemplar: simple effect size and standardized effect size 2.1.6 How do I know my effect is large enough? Although there exist rules of thumb to help interpret standardized effect sizes, these are not universally accepted. See What about Cohen’s small, medium, and large effect sizes? It is generally advisable to avoid the use of arbitrary thresholds when deciding on whether an effect is large enough, and instead try to think of whether the effect is of practical importance. This requires domain knowledge, and often a fair degree of subjective judgment. Ideally, a researcher should think in advance what effect size they would consider to be large enough, and plan the experiment, the hypotheses and the analyses accordingly (see the experiment and analysis planning FAQ). Nevertheless, more often than not in HCI, it is difficult to determine whether a certain effect is of practical importance. For example, a difference in pointing time of 100 ms between two pointing techniques can be large or small depending on the application, how often it is used, its context of use, etc. In such cases, forcing artificial interpretations of practical importance can hurt transparency. In many cases, it is sufficient to present effect sizes in a clear manner and leave the judgment of practical importance to the reader. Simple effect sizes provide the information necessary for an expert in the area to use their judgment to assess the practical impact of an effect size. For example, a difference in reaction time of 100ms is above the threshold of human perception, and therefore likely of practical impact. A difference of 100ms in receiving a chat message in an asynchronous chat application is likely less impactful, as it is small compared to the amount of time a chat message is generally expected to take. Presenting simple effect sizes in a clear way—with units—allows the expert author to argue why the effect size may or may not have practical importance and allow the expert reader to make their own judgment. For the judgement about standardized effect sizes, see the next section. 2.1.7 What about Cohen’s small, medium, and large effect sizes? Conventional thresholds are sometimes used for standardized effect sizes like Cohen’s d, labeling them “small”, “medium”, or “large”. These thresholds are however largely arbitrary (Cummings 2011). They were originally proposed by Cohen based on human heights and intelligence quotients (Cohen 1977), but Cohen, in the very text where he first introduced them, noted that these thresholds may not be directly applicable to other domains: The terms “small”, “medium”, and “large” are relative, not only to each other, but to the area of behavioral science or even more particularly to the specific content and research method being employed in any given investigation… In the face of this relativity, there is a certain risk inherent in offering conventional operational definitions for these terms for use in power analysis in as diverse a field of inquiry as behavioral science. This risk is nevertheless accepted in the belief that more is to be gained than lost by supplying a common conventional frame of reference which is recommended for use only when no better basis for estimating the ES index is available. (Cohen 1977) Cohen recommended the use of these thresholds only when no better frame of reference for assessing practical importance was available. However, hindsight has demonstrated that if such thresholds are offered, they will be adopted as a convenience, often without much thought to how they apply to the domain at hand (Baguley 2004; Lenth 2001); Lenth has called this usage “canned effect sizes” (Lenth 2001). Once adopted, these thresholds make reports more opaque, by standardizing away units of measurement and categorizing results into arbitrary classes. Such classes can even be misleading. For example, a review of 92 experiments shows that effect sizes in software engineering are larger than Cohen’s categories (Kampenes et al. 2007). Like Cummings (2011), we recommend against assessing the importance of effects by labeling them using Cohen’s thresholds. 2.1.8 How to use effect sizes in planning a study? See the Experiment and analysis planning FAQ. 2.1.9 What are controversial issues about effect sizes? Are effect sizes practical for lab experiments? Read about the argument and a response. Simple vs. standardized effect sizes: See further disucssion here. 2.2 Exemplar: Simple effect size This section is in alpha. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. 2.2.1 Data Imagine a between-subjects design, with completion time (in milliseconds) measured in two groups, A and B, with 20 subjects each. A good first step in any analysis is always to visualize the raw data along with some relvant descriptive statistics (see the Statistical Inference chapter). In this case, we will use a dotplot (Wilkinson 1999b) to show all the raw data, and we will indicate the mean completion time as a red dotted vertical line: 2.2.2 Calculating simple effect size Since we have meaningful units (milliseconds), we will use the difference in mean completion time as our effect size. Following our recommendations on how to report effect size, we also need to report the uncertainty around the sample effect size. There are several possible approaches to estimating the uncertainty in the difference between the two groups. For simplicity, we show one possible approach in this exemplar, but we provide a non-exhaustive comparison of a few other approaches in the effect size guideline appendix. While the response distributions are non-normal, the sampling distribution of the difference in means will still be defined on \\((-\\infty, +\\infty)\\) and approximately symmetrical (per the central limit theorem), so we will compute a Student’s t distribution confidence interval for the difference in means. 2.2.3 Reporting simple effect size 2.2.3.1 Graphical report Ideally, we would have space in our paper to report the effect size graphically. Here, we will show 66% and 95% confidence intervals around the mean difference, along with a sampling distribution for the mean difference: This graphical report includes all of the elements of an effect size report that we recommend: The direction of the difference (indicated by the label A - B) The type of estimate reported (mean difference) The type of uncertainty reported (66% CI, 95% CI, and sampling distribution) The units (ms) Space permitting, the raw data and the effect size may be combined into a single plot with align scales, so that the effect size and its uncertainty have the full context of the raw data: 2.2.3.2 Textual report Space may not always permit a graphical report. While it can be less easy to interpret, an alternative is a textual report. Such a report should still include all of the four elements listed above. For example: Group A had a greater mean completion time than group B by 104 milliseconds (95% CI: [81, 126]). This report includes: The direction of the difference (indicated by “Group A had greater mean…”) The type of estimate reported (difference in mean completion time) The type of uncertainty (95% CI) The units (milliseconds) 2.2.4 Interpreting effect size: same result, different domains = different interpretations Because simple effect sizes include units, we can use our expert judgment to interpret the report. Authors may wish to do so in order to put their result in context. Because the report above includes everything necessary for other experts to come to their own conclusion, providing our own interpretation does not prevent readers from applying their own judgment and coming to different conclusions. To illustrate the effect of domain on interpreting effect size, we will imagine two different domains that might have led to the same result reported above, and write a different interpretation of the data for each. 2.2.4.1 Domain 1: Physical prototyping Imagine the above study was from the comparison of a novel physical user interface prototyping system (treatment B) to the previous state of the art (A), and the completion time referred to the time for feedback to be given to the user after they perform an input action. We might report the following interpretation of the results: Technique B offers a large improvement in feedback time (~81 – 126ms mean decrease), resulting in feedback times that tend to be less than the threshold of human perception (less than about 100ms). By contrast, the larger feedback times offered by technique A tended to be above that threshold, possibly degrading users’ experience of the prototypes built using that technique. 2.2.4.2 Domain 2: Chatbots Imagine the same quantitative results, now in the context of a natural language chat bot designed to answer users’ questions. Here, technique A will be the novel system, with improved natural language capabilities compared to the previous state-of-the-art technique, B. We might report the following interpretation of the results: While technique A takes longer to respond to chat messages (~81–126ms increase in mean response time), we believe this difference is acceptable in the context of an asynchronous chat interface in which users do not expect instantaneous responses. When weighed against the improved natural language capabilites of technique A, we believe this small increase in response time for messages is worth the improved message content. The same effect size is plausibly described as large in domain 1 and small in domain 2, illustrating the importance of expert interpretation to reporting and understanding effect size and the difficulty in applying pre-defined thresholds across domains. 2.3 Exemplar: Within-subjects experiment This section is in alpha. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. Large individual differences can be a major source of noise. An effective way of accounting for that noise is for every subject to run in every combination of conditions multiple times. This “within-subject” experiment design combined with many repetitions per condition can substantially reduce any noise from individual differences, allowing for more precise measurements despite a small number of subjects. In this example, we’ll pretend we’ve run an experiment that compared different interfaces for visualizing data. Here are the parameters that we manipulate in the experiment: Independent Variable layout: the two layouts of the interface Independent Variable size: the size of the dataset visualized (small, medium, and large) Independent Variable color: interface color, where we don’t expect any effect We run each subject through each combination of these variables 20 times to get (2 layouts) × (3 sizes) × (4 colors) × (20 repetitions) = 480 trials per subject. We measure some response (e.g., response time) in each trial. 2.3.1 Libraries needed for this analysis library(tidyverse) library(afex) # for aov_ez() library(parallel) # for parLapply() 2.3.2 Simulate a dataset 2.3.2.1 Subjects, conditions, and repetitions In this example, there are 6 subjects (subject column). set.seed(543) # make the output consistent SUBJECT_COUNT = 6 data &lt;- expand.grid( subject = paste(&#39;Subject&#39;, LETTERS[1:SUBJECT_COUNT]), # subject IDs layout = 0:1, # independent variable size = 0:2, # independent variable color = 0:3, # independent variable repetition = 1:20 # each subject runs in each condition multiple times ) # peak at the data head(data) subject layout size color repetition Subject A 0 0 0 1 Subject B 0 0 0 1 Subject C 0 0 0 1 Subject D 0 0 0 1 Subject E 0 0 0 1 Subject F 0 0 0 1 2.3.2.2 Individual differences Not all subjects behave the same way. Some people might be tired, bad at the task, or just not trying very hard. These performance differences can’t be directly measured, but they can substantially impact the results. We’ll simulate these individual differences by giving each subject a random performance handicap. # build a table of subject performance multipliers individualDifferences &lt;- tibble(subject = unique(data$subject)) individualDifferences$handicap &lt;- rnorm(SUBJECT_COUNT, 20, 4) # individual differences # put it in the main dataset data &lt;- data %&gt;% left_join(individualDifferences, by = &quot;subject&quot;) 2.3.2.3 Simulate some noisy effects We’ll simulate an experiment with a main effect of layout and size and an interaction between them. However, color and its interactions will not have an impact. # simulate the response times with a clean model data &lt;- data %&gt;% mutate( response_time = layout * .4 + # main effect of layout size * .2 + # main effect of size color * 0 + layout * size * .6 + # 2-way interaction size * color * 0 + layout * color * 0 + layout * size * color * 0 ) # add some reponse noise data &lt;- data %&gt;% mutate(response_time = response_time + rnorm(n())) # add noise from individual handicaps data &lt;- data %&gt;% mutate(response_time = 30 + handicap*2 + response_time * handicap) Even though we used numbers to simulate the model, the independent variables and subject ID are all factors. data &lt;- data %&gt;% mutate( subject = factor(subject), layout = factor(layout), color = factor(color) ) 2.3.3 A look at the data Let’s get an overview of the results by graphing each subject’s average response time for each condition. data %&gt;% group_by(layout, size, color, subject) %&gt;% summarize(response_time = mean(response_time)) %&gt;% ggplot() + aes(y=response_time, x=size, linetype=layout, color=color, group=paste(layout,color,subject)) + geom_line(size=1.5) + scale_color_brewer(palette=&#39;Set1&#39;) + facet_wrap(~subject) + labs(title=&#39;Response times for each subject&#39;) + theme_bw() Despite a lot of variability in raw values between subjects (individual differences), we can see some consistent patterns. The dashed lines are higher (main effect) and more sloped (interaction) than the solid lines. But there doesn’t seem to be any consistent ordering of the colors. 2.3.4 Compute effect sizes While Cohen’s d is often used for simple 2-factor, single-trial, between-subject designs, repetition skews the measure to be very high. Experiment results with lots of repetition can be more reliably interpreted with the eta squared (\\(\\eta^{2}\\)) family of effect sizes, which represent the proportion of variance accounted for by a particular variable. A variant, generalized eta squared (\\(\\eta_{G}^{2}\\)), is particularly suited for providing comparable effect sizes in both between and within-subject designs (Olejnik and Algina 2003; Bakeman 2005). This property makes it more easily applicable to meta-analyses. For those accustomed to Cohen’s d, it’s important to be aware that \\(\\eta_{G}^{2}\\) is typically smaller, with a Cohen’s d of 0.2 being equivalent to a \\(\\eta_{G}^{2}\\) of around 0.02. Also, the actual number has little meaning beyond its scale relative to other effects. results = afex::aov_ez( data = data, id = &#39;subject&#39;, # subject id column dv = &#39;response_time&#39;, # dependent variable within = c(&#39;layout&#39;, &#39;size&#39;, &#39;color&#39;), # within-subject independent variables between = NULL ,# between-subject independent variables fun_aggregate = mean, # average multiple repetitions together for each subject*condition anova_table = list(es = &#39;ges&#39;) # effect size = generalized eta squared ) Note: fun_aggregate = mean collapses repetitions into a mean, which may be a problem if an experiment is not fully counterbalanced. This example, however, has every subject running in every combination of conditions, so simple collapsing is the correct procedure. anova_results &lt;- results$anova_table %&gt;% rownames_to_column(&#39;effect&#39;) %&gt;% # put effect names in a column select(-`Pr(&gt;F)`) # no need to show p-values anova_results %&gt;% as.tibble() ## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. effect num Df den Df MSE F ges layout 1 5 86.61117 186.9153824 0.5489428 size 2 10 51.97262 108.2961322 0.4583589 color 3 15 14.50618 0.9125176 0.0029764 layout:size 2 10 31.84102 59.8437233 0.2226922 layout:color 3 15 19.24758 0.1492400 0.0006474 size:color 6 30 21.02831 0.4695047 0.0044335 layout:size:color 6 30 20.73981 1.4065459 0.0129870 Looking at the F and ges (generalized eta squared) columns, layout and size and the interaction between layout and size account for much more of the noise than color and the other 2-way and 3-way interactions do. 2.3.5 Bootstrapping Draft. Needs work. But that only gives us one one point estimate per effect, whereas we want a confidence interval. We’ll use a technique called bootstrapping, which checks the effect size for many randomized samples of the data. Importantly, bootstrapping samples “with replacement”, meaning that items can be sampled more than once or not at all. If a small subset of the observations are driving an effect, they won’t impact all of the samples. Consequently, the spread of the bootstrapped confidence intervals shows how consistent the results are for different samples. Randomly sample with replacement (meaning the same value might be sampled more than once) to build a new dataset Perform the analysis on this new dataset Do that many times Sort the results for each effect and find the 95% confidence interval 2.3.5.1 Prepare for bootstrapping # data used for bootstrapping will collapse by each subject-condition combination data_aggregated &lt;- data %&gt;% group_by(layout, size, color, subject) %&gt;% summarize(response_time = mean(response_time)) 2.3.5.2 Each iteration Each iteration of the bootstrap samples the original dataset and runs the analysis on this permutation. # run one iteration of the bootstrap procedure analyze_one_iteration &lt;- function(x) { subjects &lt;- unique(data_aggregated$subject) # select subjects at random with replacement sampled_subjects &lt;- sample(subjects, length(subjects), replace=TRUE) # get all the results for one subject # and give them a new unique subject id get_one_subjects_data &lt;- function(i) { data_aggregated %&gt;% filter(subject == sampled_subjects[i]) %&gt;% mutate(subject = paste(sampled_subjects[i], i)) } # get all of the boostrapped subjects&#39; data boot_data &lt;- lapply(1:length(sampled_subjects), get_one_subjects_data) %&gt;% bind_rows() # compute the effect sizes the same way we did without bootstrapping afex::aov_ez( data = boot_data, id = &#39;subject&#39;, # subject id column dv = &#39;response_time&#39;, # dependent variable within = c(&#39;layout&#39;, &#39;size&#39;, &#39;color&#39;), # within-subject independent variables between = NULL ,# between-subject independent variables #fun_aggregate = mean, anova_table = list(es = &#39;ges&#39;) # effect size = generalized eta squared )$anova_table %&gt;% as.tibble() %&gt;% rownames_to_column(&#39;effect&#39;) %&gt;% # put effect names in a column return() } 2.3.5.3 Iterate The bootstrap needs to run many times to determine how a subset of the data impacts # run many iterations of the bootstrap procedure analyze_many_iterations = function (bootstrap_iteration_count) { # each core needs to reload the libraries library(tidyverse) library(afex) lapply(1:bootstrap_iteration_count, function(x) analyze_one_iteration(x)) %&gt;% bind_rows() } 2.3.5.4 Parallelize Bootstrapping can be slow, especially with thousands of iterations. Splitting the iterations across processor cores cuts down on the wait time. BOOTSTRAP_COUNT &lt;- 100 # at least 5k recommended. Use lower values for quicker testing. # Initiate cluster core_count &lt;- detectCores() - 1 core_count &lt;- ifelse(core_count &lt; 1, 1, core_count) my_cluster &lt;- makeCluster(core_count) # make sure each core in the cluster defines these functions clusterExport(my_cluster, &quot;analyze_one_iteration&quot;) clusterExport(my_cluster, &quot;data_aggregated&quot;) # how many times should each core iterate bootstrap_iteration_count &lt;- BOOTSTRAP_COUNT / core_count # run the bootstrap and output the time system.time( boot_results &lt;- parLapply( my_cluster, # the cluster of cores rep(bootstrap_iteration_count, core_count), # how many runs for each core analyze_many_iterations) # the function to run in each core ) ## user system elapsed ## 0.004 0.000 8.873 # the cluster is no longer needed stopCluster(my_cluster) # cleanup rm(core_count, my_cluster, bootstrap_iteration_count, data_aggregated) 2.3.6 Getting a confidence interval from a bootstrap Each bootstrap iterations ran one anaylsis, so wwe now have many results. So for each effect size, we sort the results and find the range of the inner 95% percentiles. # inner 95% PERCENTILE_LO &lt;- 0.025 PERCENTILE_HI &lt;- 0.975 # put all the boostraped results together boot_results &lt;- bind_rows(boot_results) boot_results &lt;- boot_results %&gt;% group_by(effect) %&gt;% summarize( effectsize_conf_low = unname(quantile(ges, probs = PERCENTILE_LO)), effectsize_conf_high = unname(quantile(ges, probs = PERCENTILE_HI))) # add the low and hi end estimates to the effect size table anova_results &lt;- anova_results %&gt;% left_join(boot_results, by = &#39;effect&#39;) # show the table anova_results %&gt;% as.tibble() effect num Df den Df MSE F ges effectsize_conf_low effectsize_conf_high layout 1 5 86.61117 186.9153824 0.5489428 NA NA size 2 10 51.97262 108.2961322 0.4583589 NA NA color 3 15 14.50618 0.9125176 0.0029764 NA NA layout:size 2 10 31.84102 59.8437233 0.2226922 NA NA layout:color 3 15 19.24758 0.1492400 0.0006474 NA NA size:color 6 30 21.02831 0.4695047 0.0044335 NA NA layout:size:color 6 30 20.73981 1.4065459 0.0129870 NA NA 2.3.7 Plot the effect sizes anova_results %&gt;% # reverse order the factors so that they appear in proper order in the plot mutate(effect = fct_rev(fct_inorder(effect))) %&gt;% # plot and mapping ggplot(aes(x = effect, y = ges, ymin = effectsize_conf_low, ymax = effectsize_conf_high)) + # reference line of no-effect geom_hline(yintercept = 0, linetype = &#39;dotted&#39;) + # point- and interval-estimates geom_pointrange() + # ensures that we see the reference line expand_limits(x = 0) + # labels labs(y = expression(paste(&#39;Effect size &#39;, eta[G]^2))) + # flip to horizontal plot and apply black-and-white theme coord_flip() + theme_bw() ## Warning: Removed 7 rows containing missing values (geom_pointrange). 2.3.8 Reporting the results Generalized eta squared (GES) represents the proportion of variance in the results explained by each variable. The previous graph shows clear main effects for layout and size and an interaction between layout and size. However color and the other 2-way and 3-way interactions are relatively much smaller, barely above zero. There is no useful cutoff for what counts as a “significant” effect, so think in terms of relative size – which variables best explain the variance in the results? Strong effects: layout: F1,5 = 187, \\(\\eta_{G}^{2}\\) = 0.549 95% CI [NA, NA] size: F2,10 = 108, \\(\\eta_{G}^{2}\\) = 0.458 95% CI [NA, NA] layout × size: F2,10 = 59.8, \\(\\eta_{G}^{2}\\) = 0.223 95% CI [NA, NA] Minimally impactful: color F3,15 = 0.913, \\(\\eta_{G}^{2}\\) = 0.00298 95% CI [NA, NA] layout × color: F3,15 = 0.149, \\(\\eta_{G}^{2}\\) = 0.000647 95% CI [NA, NA] size × color: F6,30 = 0.47, \\(\\eta_{G}^{2}\\) = 0.00443 95% CI [NA, NA] layout × size × color: F6,30 = 1.41, \\(\\eta_{G}^{2}\\) = 0.013 95% CI [NA, NA] 2.4 Exemplar: Standardized effect size This section is in alpha. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. TODO: This needs a domain where we can argue that Cohen&#39;s d is an exemplar analysis, then repeat structure of exemplar 1 with it May be an example of existing meta-analysis in HCI. 2.4.1 Libraries needed for this analysis library(tidyverse) library(effsize) # for cohen.d() 2.4.2 Standardized effect size set.seed(12) n &lt;- 20 data &lt;- tibble( group = rep(c(&quot;A&quot;, &quot;B&quot;), each = n), completion_time_ms = c( rlnorm(n, meanlog = log(170), sdlog = 0.3), rlnorm(n, meanlog = log(50), sdlog = 0.4) ) ) cohen_d &lt;- cohen.d(completion_time_ms ~ group, data = data) # manual calculation data_A &lt;- (data %&gt;% filter(group == &quot;A&quot;))[[&quot;completion_time_ms&quot;]] data_B &lt;- (data %&gt;% filter(group == &quot;B&quot;))[[&quot;completion_time_ms&quot;]] sd_A &lt;- sd(data_A) sd_B &lt;- sd(data_B) sd_pool &lt;- sqrt( (sd_A^2 + sd_B^2) / 2 ) cohen_d_manual &lt;- abs(mean(data_A) - mean(data_B))/sd_pool Standardized effect size: Cohen’s d = 2.97 SDs with 95% confidence interval [2.04 , 3.90] 2.5 Exemplar: Nonparametric effect size This section is in alpha. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. For a nonparametric test that produces a Z-score, like the Mann-Whitney U test or the Wilcoxon Signed-Rank test, an effect size can be computed as: \\(r = \\left|\\frac{Z}{\\sqrt{N}}\\right|\\) Above, Z is the Z-score and N is the number of observations in all groups [Rosenthal (1991), p. 19). The result, r, is a variance-based effect size, like Pearson r, not a Cohen d-family effect size. The r can be squared to estimate the percentage of variance explained, however it will not be exactly equivalent to the Pearson r. TODO: This needs a domain where we can argue that the nonparametric approach is an exemplar analysis, then repeat structure of exemplar 1 with it 2.5.1 Libraries needed for this analysis library(tidyverse) library(coin) # for wilcox_test 2.5.2 Nonparametric effect size set.seed(12) n &lt;- 20 data &lt;- tibble( group = rep(c(&quot;A&quot;, &quot;B&quot;), each = n), completion_time_ms = c( rlnorm(n, meanlog = log(170), sdlog = 0.3), rlnorm(n, meanlog = log(50), sdlog = 0.4) ) ) data_A &lt;- (data %&gt;% filter(group == &quot;A&quot;))[[&quot;completion_time_ms&quot;]] data_B &lt;- (data %&gt;% filter(group == &quot;B&quot;))[[&quot;completion_time_ms&quot;]] wilcox_result &lt;- wilcox_test(completion_time_ms ~ factor(group), data = data) effect_r &lt;- abs(wilcox_result@statistic@teststatistic / sqrt(nrow(data))) Non-parametric effect size: Variance-based effect size r = 0.84. References "],
["pvalues.html", "Chapter 3 p values", " Chapter 3 p values This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["inference.html", "Chapter 4 Inferential statistics", " Chapter 4 Inferential statistics This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["interpretations.html", "Chapter 5 Interpretations and Conclusion", " Chapter 5 Interpretations and Conclusion This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["preregistration.html", "Chapter 6 Preregistration and prespecification", " Chapter 6 Preregistration and prespecification This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["multiplecomparisons.html", "Chapter 7 Multiple comparisons", " Chapter 7 Multiple comparisons This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["interrater.html", "Chapter 8 Inter-rater reliability", " Chapter 8 Inter-rater reliability This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["modelselection.html", "Chapter 9 Model selection and evaluation", " Chapter 9 Model selection and evaluation This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["bayesian.html", "Chapter 10 Bayesian inference", " Chapter 10 Bayesian inference This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["likert.html", "Chapter 11 Likert-scale data", " Chapter 11 Likert-scale data This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["assumptions.html", "Chapter 12 Assumptions and robustness", " Chapter 12 Assumptions and robustness This section is currently under draft on this Google Doc. We welcome help and feedback at all levels! If you would like to contribute, please see Contributing to the Guidelines. "],
["appendix-effectsize.html", "A Effect size appendix A.1 Alternative approaches for simple effect size exemplar", " A Effect size appendix This appendix contains supplements to the effect size guideline. A.1 Alternative approaches for simple effect size exemplar The simple effect size exemplar demonstrates one common technique for esitmating mean differences in resposne time and the uncertainty around them (Student’s t confidence intervals). This supplement demonstrates several possible approaches one might take to calculate differences in response time, and compares them. It is not intended to be exhaustive. A.1.1 Libraries needed for this analysis # See here for rstan installation instructions: # https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started library(rstan) library(tidyverse) library(modelr) # for data_grid() library(broom) # for tidy() library(ggstance) # for geom_pointrangeh(), stat_summaryh() library(brms) # for brm() (requires rstan) library(tidybayes) # for mean_qi() # requires `import` and `MASS` packages to be installed import::from(MASS, mvrnorm) A.1.2 Data We will use the same data as the simple effect size exemplar: set.seed(12) n &lt;- 20 data &lt;- tibble( group = rep(c(&quot;A&quot;, &quot;B&quot;), each = n), completion_time_ms = c( rlnorm(n, meanlog = log(170), sdlog = 0.3), rlnorm(n, meanlog = log(50), sdlog = 0.4) ) ) See that exemplar for more information. A.1.3 Calculating simple effect size A.1.3.1 Approach 1: Difference in means with Student’s t confidence interval This is the approach used in the exemplar. While the response distributions are non-normal, the sampling distribution of the difference in means will still be defined on \\((-\\infty, +\\infty)\\) and approximately symmetrical (per the central limit theorem), so we can compute a Student’s t distribution confidence interval for the difference in means. t_interval &lt;- t.test(completion_time_ms ~ group, data = data) %&gt;% tidy() # put result in tidy tabular format t_interval estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative 103.6021 159.0898 55.48774 9.388748 0 28.66574 81.02211 126.182 Welch Two Sample t-test two.sided The tidy()ed output of the t.test() function includes an estimate of the mean difference in milliseconds (estimate) as well as the lower (conf.low) and upper (conf.high) bounds of the 95% confidence interval. A.1.3.2 Approach 2a: Ratio of geometric means with Student’s t confidence interval on log-scale For responses that are assumed to be log-normal, one alternative is to calculate the mean difference on the log scale. Because the mean on the log scale corresponds to the geometric mean of the untransformed responses, this is equivalent to calculating a ratio of geometric means on the untransformed scale (in this case, a ratio of geometric mean response times). See the data transformation guideline for more information. log_t_interval &lt;- t.test(log(completion_time_ms) ~ group, data = data) %&gt;% tidy() # put result in tidy tabular format log_t_interval estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative 1.085344 5.036436 3.951092 11.05821 0 34.90244 0.8860725 1.284615 Welch Two Sample t-test two.sided We can also transform this difference (in the log scale) into a ratio of geometric mean response times: log_t_ratios &lt;- log_t_interval %&gt;% mutate_at(vars(estimate, estimate1, estimate2, conf.low, conf.high), exp) log_t_ratios estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative 2.960458 153.9204 51.9921 11.05821 0 34.90244 2.425584 3.613278 Welch Two Sample t-test two.sided This output shows the estimated geometric mean response times (estimate1 and estimate2), and an estimate of the ratio between them (estimate = estimate1/estimate2) as well as the lower (conf.low) and upper (conf.high) bounds of the 95% confidence interval of that ratio. This allows us to estimate how many times slower or faster one condition is compared to another. However, since we have some sense in this context of how large or small we might want response times to be on the original scale (e.g., people tend to perceive differences on the order of 100ms), it may be easier to interpret effect sizes if we calculate them on that scale. In this case, the geometric mean of condition A is roughly 154 and of B is roughly 52.0. A is about 2.96 \\(\\times\\) B, with a 95% confidence interval of [2.43\\(\\times\\), 3.61\\(\\times\\)]. So we have: 52.0 \\(\\times\\) 2.96 \\(\\approx\\) 154. A.1.3.3 Approach 2b: log-normal regression with marginal estimate of difference in means using simulation We can run a linear regression that is equivalent to approach 2a: m_log &lt;- lm(log(completion_time_ms) ~ group, data = data) summary(m_log) ## ## Call: ## lm(formula = log(completion_time_ms) ~ group, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.49993 -0.18776 0.00806 0.12970 0.78975 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.03644 0.06940 72.57 &lt; 2e-16 *** ## groupB -1.08534 0.09815 -11.06 1.94e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3104 on 38 degrees of freedom ## Multiple R-squared: 0.7629, Adjusted R-squared: 0.7567 ## F-statistic: 122.3 on 1 and 38 DF, p-value: 1.939e-13 This model estimates the geometric means in each group. However, we want to know the difference in means on the original (time) scale, not on the log scale. We can translate the log-scale means into means on the original scale using the fact that if a random variable \\(X\\) is log-normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\): \\[ \\log(X) \\sim \\mathrm{Normal}(\\mu, \\sigma^2) \\] Then the mean of \\(X\\) is (see here): \\[ \\mathbf{E}[X] = e^{\\mu+\\frac{\\sigma^2}{2}} \\] We will use the sampling distribution of the coefficients of m_log to generate samples of \\(\\mu\\) in each group, then translate these samples (along with an estimate of \\(\\sigma\\)) onto the outcome scale. Given an estimate of the coefficients (\\(\\boldsymbol{\\hat\\beta}\\)) and an estimate of the covariance matrix (\\(\\boldsymbol{\\hat\\Sigma}\\)), the sampling distribution of the coefficients on a log scale is a multivariate normal distribution: \\[ \\mathrm{Normal}(\\boldsymbol{\\hat\\beta}, \\boldsymbol{\\hat\\Sigma}) \\] We can sample from that distribution and use the estimated log-scale standard deviation (\\(\\hat\\sigma\\)) to generate sample means on the untransformed scale, which we can use to derive a difference of means on the original scale and a confidence interval around that difference (this is sort of treating the sampling distribution as a Bayesian posterior): log_interval &lt;- mvrnorm(10000, mu = coef(m_log), Sigma = vcov(m_log)) %&gt;% as_data_frame() %&gt;% mutate( sigma = sigma(m_log), # Using MLE estimate of residual SD. Could also sample from # sqrt(rgamma(nrow(.), (n - 1)/2, ((n - 1)/sigma(m_log)^2)/2)) # but results are similar # get samples of means for each group on the original scale mu_A = `(Intercept)`, mean_A = exp(mu_A + sigma^2 / 2), mu_B = `(Intercept)` + groupB, mean_B = exp(mu_B + sigma^2 / 2), # get samples of the difference in means on the original scale estimate = mean_A - mean_B ) %&gt;% mean_qi(estimate) %&gt;% to_broom_names() %&gt;% # makes the column names the same as those returned by broom::tidy mutate(method = &quot;lognormal regression&quot;) ## Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. log_interval estimate conf.low conf.high .width .point .interval method 107.2028 85.23455 131.7186 0.95 mean qi lognormal regression This approach does not account for non-constant variance on the log scale, however; i.e., the fact that the variances of the two groups are different. The next approach does. A.1.3.4 Approach 3a: log-normal regression with marginal estimate of difference in means using Bayesian regression (uninformed priors) For this approach, we will use a Bayesian log-normal regression model to estimate the mean and variance of the response distribution in each group on a log scale. We will then transform these parameters into a difference in means on the original (millisecond) scale, as in approach 2b. For this approach, we will use a Bayesian log-normal regression with uninformed priors. This model is the same as the lm model in approach 2, except that it also allows the variance to be different in each group (in other words, it does not assume constant variance between groups, also known as homoskedasticity). m_log_bayes &lt;- brm(brmsformula( completion_time_ms ~ group, sigma ~ group # allow variance to be different in each group ), data = data, family = lognormal) Similar to approach 2b, we will derive samples of the mean difference, this time from the posterior distribution. We will use these to derive a credible interval (Bayesian analog to a confidence interval) around the mean difference: log_bayes_samples &lt;- m_log_bayes %&gt;% tidy_draws() %&gt;% mutate( mu_A = b_Intercept, sigma_A = exp(b_sigma_Intercept), mean_A = exp(mu_A + sigma_A^2 / 2), mu_B = b_Intercept + b_groupB, sigma_B = exp(b_sigma_Intercept + b_sigma_groupB), mean_B = exp(mu_B + sigma_B^2 / 2), estimate = mean_A - mean_B ) log_bayes_interval &lt;- log_bayes_samples %&gt;% mean_qi(estimate) %&gt;% to_broom_names() %&gt;% mutate(method = &quot;lognormal regression (Bayesian, uninformed)&quot;) log_bayes_interval estimate conf.low conf.high .width .point .interval method 103.923 82.73573 128.301 0.95 mean qi lognormal regression (Bayesian, uninformed) Alternatively, we could use tidybayes::add_fitted_draws, which internally calls brmsfit::posterior_linpred, which does the same math as above to calculate the posterior distribution for the mean on the response scale. This saves us some math (and makes sure we did not do that math incorrectly): log_bayes_samples &lt;- data %&gt;% # reverse the order of group so that the output is A - B instead of B - A data_grid(group = fct_rev(group)) %&gt;% add_fitted_draws(m_log_bayes, value = &quot;estimate&quot;) %&gt;% ungroup() %&gt;% compare_levels(estimate, by = group) %&gt;% mutate(method = &quot;lognormal regression (Bayesian, uninformed)&quot;) %&gt;% group_by(method) log_bayes_interval &lt;- log_bayes_samples %&gt;% mean_qi(estimate) %&gt;% to_broom_names() log_bayes_interval method estimate conf.low conf.high .width .point .interval lognormal regression (Bayesian, uninformed) 103.923 82.73573 128.301 0.95 mean qi This gives the estimated mean difference between conditions in milliseconds (estimate), as well as the lower (conf.low) and upper (conf.high) bounds of the 95% quantile credible interval of that difference. A.1.3.5 Approach 3b: log-normal regression with marginal estimate of difference in means using Bayesian regression (weakly informed priors) Finally, let’s run the same analysis with weakly informed priors based on what we might believe reasonable ranges of the effect are. To see what priors we can set in brm models, we can use the get_prior() function: get_prior(brmsformula( # for using informed priors, the `0 + intercept` formula can be helpful: otherwise, # brm re-centers the data on 0, which can make it harder to set an informed prior. # see help(&quot;set_prior&quot;) for more information. completion_time_ms ~ group + 0 + intercept, sigma ~ group + 0 + intercept ), data = data, family = lognormal) prior class coef group resp dpar nlpar bound b b groupB b intercept b sigma b groupB sigma b intercept sigma This shows priors on the log-scale mean and priors on the log-scale standard deviation (sigma). First, we’ll assume that completion time is something like a pointing task: not reasonably faster than 10ms or slower than 2s (2000ms). On log scale, that is between approximately \\(\\log(10) \\approx 2\\) and \\(\\log(2000) \\approx 8\\), so we’ll center our prior intercept between these (\\((8+2)/2\\)) and give it a 95% interval that covers them (sd of \\((8-2)/4\\)): \\(\\mathrm{Normal}(5, 1.5)\\). For differences in log mean, we’ll assume that times will not be more than about 100\\(\\times\\) difference in either direction: a zero-centered normal prior with standard deviation \\(\\approx log(100)/2 \\approx 2.3\\): \\(\\mathrm{Normal}(0, 2.3)\\). Since the standard deviation is estimated using a submodel that itself uses a log link, we have to make a prior on the log scale of log standard deviation. For standard deviation on the log scale, let’s assume a baseline of around 100ms response time. Then, our prior on standard deviation on the log scale could reasonbly be as small as one reflecting individual differences on the order of 10ms: \\(log(110) - log(100) \\approx 0.1 \\approx e^-2.4\\), or as large as one reflecting a difference of 1 second: \\(log(1100) - log(100) \\approx 2.4 \\approx e^0.9\\). So we’ll center our log log sd prior at \\((0.9 + -2.4)/2\\) and give it a 95% interval that covers them (sd of \\((0.9 - -2.4)/4\\)): \\(\\mathrm{Normal}(-0.75, 0.825)\\). Finally, for differences in log log standard deviation, we’ll assume zero-centered with similar magnitude to the intercept: \\(\\mathrm{Normal}(0, 0.825)\\). These priors can be specified as follows: log_bayes_priors &lt;- c( prior(normal(5.5, 1.75), class = b, coef = intercept), prior(normal(0, 2.3), class = b, coef = groupB), prior(normal(-0.75, 0.825), class = b, coef = intercept, dpar = sigma), prior(normal(0, 0.825), class = b, coef = groupB, dpar = sigma) ) log_bayes_priors prior class coef group resp dpar nlpar bound normal(5.5, 1.75) b intercept normal(0, 2.3) b groupB normal(-0.75, 0.825) b intercept sigma normal(0, 0.825) b groupB sigma Then we can re-run the model from approach 3a with those priors: m_log_bayes_informed &lt;- brm(brmsformula( completion_time_ms ~ group + 0 + intercept, sigma ~ group + 0 + intercept ), data = data, family = lognormal, prior = log_bayes_priors) Similar to approach 2b, we will derive samples of the mean difference, this time from the posterior distribution. We will use these to derive a credible interval (Bayesian analog to a confidence interval) around the mean difference: log_bayes_informed_samples &lt;- data %&gt;% # reverse the order of group so that the output is A - B instead of B - A data_grid(group = fct_rev(group)) %&gt;% add_fitted_draws(m_log_bayes_informed, value = &quot;estimate&quot;) %&gt;% ungroup() %&gt;% compare_levels(estimate, by = group) %&gt;% mutate(method = &quot;lognormal regression (Bayesian, weakly informed)&quot;) %&gt;% group_by(method) log_bayes_informed_interval &lt;- log_bayes_informed_samples %&gt;% mean_qi(estimate) %&gt;% to_broom_names() log_bayes_informed_interval method estimate conf.low conf.high .width .point .interval lognormal regression (Bayesian, weakly informed) 104.4916 82.77971 129.5219 0.95 mean qi This gives the estimated mean difference between conditions in milliseconds (estimate), as well as the lower (conf.low) and upper (conf.high) bounds of the 95% quantile credible interval of that difference. A.1.4 Comparing approaches All approaches that give estimates for the difference in means give very similar results: bayes_samples = bind_rows(log_bayes_samples, log_bayes_informed_samples) bind_rows(t_interval, log_interval, log_bayes_interval, log_bayes_informed_interval) %&gt;% ggplot(aes(x = estimate, y = method)) + geom_violinh(data = bayes_samples, color = NA, fill = &quot;gray65&quot;) + geom_pointrangeh(aes(xmin = conf.low, xmax = conf.high)) + geom_vline(xintercept = 0) The Bayesian estimates include posterior distributions shown as violin plots in gray. "],
["references.html", "References", " References "]
]
