<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Transparent Statistics Guidelines</title>
  <meta name="description" content="Guidelines, FAQ, and exemplar analyses">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Transparent Statistics Guidelines" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Guidelines, FAQ, and exemplar analyses" />
  <meta name="github-repo" content="transparentstats/guidelines" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Transparent Statistics Guidelines" />
  
  <meta name="twitter:description" content="Guidelines, FAQ, and exemplar analyses" />
  

<meta name="author" content="Transparent Statistics in HCI Working Group (http://transparentstatistics.org/)">


<meta name="date" content="2018-04-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="transparent-statistics-guiding-principles.html">
<link rel="next" href="bayesian.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Transparent Statistics Guidelines</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html"><i class="fa fa-check"></i><b>1</b> Transparent Statistics Guiding Principles</a><ul>
<li class="chapter" data-level="1.1" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#guiding-principles"><i class="fa fa-check"></i><b>1.2</b> Guiding Principles</a><ul>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#faithfulness"><i class="fa fa-check"></i>1. Faithfulness</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#robustness"><i class="fa fa-check"></i>2. Robustness</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#resilience"><i class="fa fa-check"></i>3. Resilience</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#process-transparency"><i class="fa fa-check"></i>4. Process Transparency</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#clarity"><i class="fa fa-check"></i>5. Clarity</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#simplicity"><i class="fa fa-check"></i>6. Simplicity</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#non-contingency"><i class="fa fa-check"></i>7. Non-contingency</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#precision-and-economy"><i class="fa fa-check"></i>8. Precision and economy</a></li>
<li class="chapter" data-level="" data-path="transparent-statistics-guiding-principles.html"><a href="transparent-statistics-guiding-principles.html#material-availability"><i class="fa fa-check"></i>9. Material availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="effectsize.html"><a href="effectsize.html"><i class="fa fa-check"></i><b>2</b> Effect size</a><ul>
<li class="chapter" data-level="2.1" data-path="effectsize.html"><a href="effectsize.html#faq"><i class="fa fa-check"></i><b>2.1</b> FAQ</a><ul>
<li class="chapter" data-level="2.1.1" data-path="effectsize.html"><a href="effectsize.html#what-is-an-effect-size"><i class="fa fa-check"></i><b>2.1.1</b> What is an effect size?</a></li>
<li class="chapter" data-level="2.1.2" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_when_why"><i class="fa fa-check"></i><b>2.1.2</b> Why and when should effect sizes be reported?</a></li>
<li class="chapter" data-level="2.1.3" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_how_reporting"><i class="fa fa-check"></i><b>2.1.3</b> How should effect sizes be reported?</a></li>
<li class="chapter" data-level="2.1.4" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_standardized"><i class="fa fa-check"></i><b>2.1.4</b> What is a standardized effect size?</a></li>
<li class="chapter" data-level="2.1.5" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_simple_v_standardized"><i class="fa fa-check"></i><b>2.1.5</b> Should simple or standardized effect sizes be reported?</a></li>
<li class="chapter" data-level="2.1.6" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_large_enough"><i class="fa fa-check"></i><b>2.1.6</b> How do I know my effect is large enough?</a></li>
<li class="chapter" data-level="2.1.7" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_small_medium_large"><i class="fa fa-check"></i><b>2.1.7</b> What about Cohen’s small, medium, and large effect sizes?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_simple"><i class="fa fa-check"></i><b>2.2</b> Exemplar: Simple effect size</a><ul>
<li class="chapter" data-level="2.2.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis"><i class="fa fa-check"></i><b>2.2.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.2.2" data-path="effectsize.html"><a href="effectsize.html#data"><i class="fa fa-check"></i><b>2.2.2</b> Data</a></li>
<li class="chapter" data-level="2.2.3" data-path="effectsize.html"><a href="effectsize.html#calculating-simple-effect-size"><i class="fa fa-check"></i><b>2.2.3</b> Calculating simple effect size</a></li>
<li class="chapter" data-level="2.2.4" data-path="effectsize.html"><a href="effectsize.html#difference-in-means-with-students-t-confidence-interval"><i class="fa fa-check"></i><b>2.2.4</b> Difference in means with Student’s t confidence interval</a></li>
<li class="chapter" data-level="2.2.5" data-path="effectsize.html"><a href="effectsize.html#reporting-simple-effect-size"><i class="fa fa-check"></i><b>2.2.5</b> Reporting simple effect size</a></li>
<li class="chapter" data-level="2.2.6" data-path="effectsize.html"><a href="effectsize.html#interpreting-effect-size-same-result-different-domains-different-interpretations"><i class="fa fa-check"></i><b>2.2.6</b> Interpreting effect size: same result, different domains = different interpretations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_within"><i class="fa fa-check"></i><b>2.3</b> Exemplar: Within-subjects experiment</a><ul>
<li class="chapter" data-level="2.3.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis-1"><i class="fa fa-check"></i><b>2.3.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="effectsize.html"><a href="effectsize.html#simulate-a-dataset"><i class="fa fa-check"></i><b>2.3.2</b> Simulate a dataset</a></li>
<li class="chapter" data-level="2.3.3" data-path="effectsize.html"><a href="effectsize.html#a-look-at-the-data"><i class="fa fa-check"></i><b>2.3.3</b> A look at the data</a></li>
<li class="chapter" data-level="2.3.4" data-path="effectsize.html"><a href="effectsize.html#compute-effect-sizes"><i class="fa fa-check"></i><b>2.3.4</b> Compute effect sizes</a></li>
<li class="chapter" data-level="2.3.5" data-path="effectsize.html"><a href="effectsize.html#bootstrapping"><i class="fa fa-check"></i><b>2.3.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="2.3.6" data-path="effectsize.html"><a href="effectsize.html#getting-a-confidence-interval-from-a-bootstrap"><i class="fa fa-check"></i><b>2.3.6</b> Getting a confidence interval from a bootstrap</a></li>
<li class="chapter" data-level="2.3.7" data-path="effectsize.html"><a href="effectsize.html#plot-the-effect-sizes"><i class="fa fa-check"></i><b>2.3.7</b> Plot the effect sizes</a></li>
<li class="chapter" data-level="2.3.8" data-path="effectsize.html"><a href="effectsize.html#reporting-the-results"><i class="fa fa-check"></i><b>2.3.8</b> Reporting the results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_standardized"><i class="fa fa-check"></i><b>2.4</b> Exemplar: Standardized effect size</a><ul>
<li class="chapter" data-level="2.4.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis-2"><i class="fa fa-check"></i><b>2.4.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.4.2" data-path="effectsize.html"><a href="effectsize.html#standardized-effect-size"><i class="fa fa-check"></i><b>2.4.2</b> Standardized effect size</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_nonparametric"><i class="fa fa-check"></i><b>2.5</b> Exemplar: Nonparametric effect size</a><ul>
<li class="chapter" data-level="2.5.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis-3"><i class="fa fa-check"></i><b>2.5.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.5.2" data-path="effectsize.html"><a href="effectsize.html#nonparametric-effect-size"><i class="fa fa-check"></i><b>2.5.2</b> Nonparametric effect size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>3</b> Bayesian inference</a></li>
<li class="chapter" data-level="4" data-path="likert.html"><a href="likert.html"><i class="fa fa-check"></i><b>4</b> Likert-scale data</a></li>
<li class="chapter" data-level="5" data-path="planning.html"><a href="planning.html"><i class="fa fa-check"></i><b>5</b> Experiment and analysis planning</a></li>
<li class="chapter" data-level="6" data-path="pvalues.html"><a href="pvalues.html"><i class="fa fa-check"></i><b>6</b> p values</a></li>
<li class="chapter" data-level="7" data-path="transformation.html"><a href="transformation.html"><i class="fa fa-check"></i><b>7</b> Data transformation</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Transparent Statistics Guidelines</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="effectsize" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Effect size</h1>
<div style="text-align:center">
<p><img src="figures/effectsize/teaser.png" /><!-- --></p>
</div>
<p><strong>Version:</strong> 1.1</p>
<p><strong>Contributed to the writing:</strong> Matthew Kay, Chat Wacharamanotham, Steve Haroz, Pierre Dragicevic, Jacob O. Wobbrock, Janne Lindqvist, Yea-Seul Kim, Amelia McNamara</p>
<p><strong>Gave feedback:</strong></p>
<p><strong>Endorsed:</strong></p>
<div id="faq" class="section level2">
<h2><span class="header-section-number">2.1</span> FAQ</h2>
<div id="what-is-an-effect-size" class="section level3">
<h3><span class="header-section-number">2.1.1</span> What is an effect size?</h3>
<p>Broadly speaking, an effect size is “anything that might be of interest” <span class="citation">(Cumming <a href="#ref-Cumming2013a">2013</a>)</span>; it is some quantity that captures the magnitude of the effect studied.</p>
<p>In HCI, common examples of effect size include the mean difference (e.g., in seconds) in task completion times between two techniques, or the mean difference in error rates (e.g., in percent). These are called <em>simple effect sizes</em> (or <em>unstandardized effect sizes</em>).</p>
<p>More complex measures of effect size exist called <em>standardized effect sizes</em> (see <a href="effectsize.html#effectsize_faq_standardized">What is a standardized effect size?</a>). Although the term <em>effect size</em> is often used to refer to standardized effect sizes only, using the term in a broader sense can avoid unnecessary confusion <span class="citation">(Cumming <a href="#ref-Cumming2013a">2013</a>; Wilkinson <a href="#ref-Wilkinson1999a">1999</a>)</span>. In this document, “effect size” refers to both simple and standardized effect sizes.</p>
</div>
<div id="effectsize_faq_when_why" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Why and when should effect sizes be reported?</h3>
<p>In quantitative experiments, effect sizes are among the most elementary and essential summary statistics that can be reported. Identifying the effect size(s) of interest also allows the researcher to turn a vague research question into a precise, quantitative question <span class="citation">(Cumming <a href="#ref-Cumming2014a">2014</a>)</span>. For example, if a researcher is interested in showing that their technique is faster than a baseline technique, an appropriate choice of effect size is the mean difference in completion times. The observed effect size will indicate not only the likely direction of the effect (e.g., whether the technique is faster or slower), but also whether the effect is large enough to care about.</p>
<p>For the sake of transparency, effect sizes should always be reported in quantitative research, unless there are good reasons not to do so. According to the American Psychological Association:</p>
<blockquote>
<p>For the reader to appreciate the magnitude or importance of a study’s findings, it is almost always necessary to include some measure of effect size in the Results section. <span class="citation">(American Psychological Association <a href="#ref-APA2001">2001</a>)</span></p>
</blockquote>
<p>Sometimes, effect sizes can be hard to compute or to interpret. When this is the case, and if the main focus of the study is on the direction (rather than magnitude) of the effect, reporting the results of statistical significance tests without reporting effect sizes (see the <a href="#inferential">inferential statistics FAQ</a>) may be an acceptable option.</p>
</div>
<div id="effectsize_faq_how_reporting" class="section level3">
<h3><span class="header-section-number">2.1.3</span> How should effect sizes be reported?</h3>
<p>The first choice is on whether to report simple effect sizes or standardized effect sizes. For this question, see <a href="effectsize.html#effectsize_faq_simple_v_standardized">Should simple effect sizes or standardized effect sizes be reported?</a></p>
<p>It is rarely sufficient to report an effect size as a single quantity. This is because a single quantity like a difference in means or a Cohen’s <em>d</em> is typically only a <em>point estimate</em>, i.e., it is merely a best guess of the true effect size. It is crucial to also assess and report the statistical uncertainty about this point estimate.</p>
<p>For more on assessing and reporting statistical uncertainty, see the <a href="#inferential">inferential statistics FAQ</a>.</p>
<p>Ideally, an effect size report should include:</p>
<ul>
<li>The direction of the effect if applicable (e.g., given a difference between two treatments <code>A</code> and <code>B</code>, indicate if the measured effect is <code>A - B</code> or <code>B - A</code>).</li>
<li>The type of point estimate reported (e.g., a sample mean difference)</li>
<li>The type of uncertainty information reported (e.g., a 95% CI)</li>
<li>The units of the effect size if applicable, or the type of standardized effect size if it is a unitless effect size.</li>
</ul>
<p>This information can be reported either numerically or graphically. Both formats are acceptable, although plots tend to be easier to comprehend than numbers when more than one effect size needs to be conveyed <span class="citation">(Loftus <a href="#ref-Loftus1993">1993</a>; Kastellec and Leoni <a href="#ref-Kastellec2007">2007</a>)</span>. Unless precise numerical values are important, it is sufficient (and often preferable) to report all effect sizes graphically. Researchers should avoid plotting point estimates without also plotting uncertainty information (using, e.g., error bars).</p>
<p>▸ Exemplar: <a href="effectsize.html#effectsize_exemplar_simple">simple effect size</a> (specifically in the “Reporting simple effect size” sub-section)</p>
</div>
<div id="effectsize_faq_standardized" class="section level3">
<h3><span class="header-section-number">2.1.4</span> What is a standardized effect size?</h3>
<p>A standardized effect size is a unitless measure of effect size. The most common measure of standardized effect size is Cohen’s <em>d</em>, where the mean difference is divided by the standard deviation of the pooled observations <span class="citation">(Cohen <a href="#ref-Cohen1988a">1988</a>)</span>. <a href="http://stats.idre.ucla.edu/other/mult-pkg/faq/general/effect-size-power/faqhow-is-effect-size-used-in-power-analysis/">Other approaches</a> to standardization exist [prefer citations]. To some extent, standardized effect sizes make it possible to compare different studies in terms of how “impressive” their results are (see <a href="effectsize.html#effectsize_faq_large_enough">How do I know my effect is large enough?</a>); however, this practice is not without criticism (see the section <em>Standardized mean differences let us compare and summarize results when studies use different outcome scales</em> of <span class="citation">(Cummings <a href="#ref-Cummings2011">2011</a>)</span>).</p>
</div>
<div id="effectsize_faq_simple_v_standardized" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Should simple or standardized effect sizes be reported?</h3>
<p>While the term <em>effect size</em> may conjure up the image of arcane statistical formulas, the most useful effect sizes are often much simpler, and more intuitive, than perhaps should even warrant a specialized term. An effect size is essentially any way to compute the practical size of an effect.</p>
<p>Standardized effect sizes are useful in some situations, for example when effects obtained from different experiments and/or expressed in different units need to be combined or compared <span class="citation">(Cumming <a href="#ref-Cumming2014a">2014</a>)</span>. However, even this practice is controversial, as it can rely on assumptions about the effects being measured that are difficult to verify <span class="citation">(Cummings <a href="#ref-Cummings2011">2011</a>)</span>.</p>
<p>In most cases, simple effect sizes should be preferred over standardized effect sizes:</p>
<blockquote>
<p>Only rarely will uncorrected standardized effect size be more useful than simple effect size. It is usually far better to report simple effect size. <span class="citation">(Baguley <a href="#ref-Baguley2009">2009</a>)</span></p>
</blockquote>
<p>Simple effect sizes are often easier to interpret and justify <span class="citation">(Cumming <a href="#ref-Cumming2014a">2014</a>; Cummings <a href="#ref-Cummings2011">2011</a>)</span>. When the units of the data are meaningful (e.g., seconds), reporting effect sizes expressed in their original units is more informative and can make it easier to judge whether the effect has a practical significance <span class="citation">(Wilkinson <a href="#ref-Wilkinson1999a">1999</a>; Cummings <a href="#ref-Cummings2011">2011</a>)</span>.</p>
<p>Barring a strong, domain- or problem-specific argument for reporting a standardized effect size instead of a simple one, simple effect sizes should be preferred as being more transparent and easier to interpret.</p>
<p>If a standardized effect size is reported, it should be accompanied by an argument for its applicability to the domain. If there is no inherent reasoning to argue for a particular interpretation of the practical significance of the standardized effect size, it should be accompanied by another assessment of the practical significance of the effect.</p>
<p>▸ Exemplar: <a href="effectsize.html#effectsize_exemplar_simple">simple effect size</a> and <a href="effectsize.html#effectsize_exemplar_standardized">standardized effect size</a></p>
</div>
<div id="effectsize_faq_large_enough" class="section level3">
<h3><span class="header-section-number">2.1.6</span> How do I know my effect is large enough?</h3>
<p>Although there exist rules of thumb to help interpret standardized effect sizes, these are not universally accepted. See <a href="effectsize.html#effectsize_faq_small_medium_large">What about Cohen’s small, medium, and large effect sizes?</a></p>
<p>It is generally advisable to avoid the use of arbitrary thresholds when deciding on whether an effect is large enough, and instead try to think of whether the effect is of practical importance. This requires domain knowledge, and often a fair degree of subjective judgment. Ideally, a researcher should think in advance what effect size they would consider to be large enough, and plan the experiment, the hypotheses and the analyses accordingly (see the <a href="planning.html#planning">experiment and analysis planning FAQ</a>).</p>
<p>Nevertheless, more often than not in HCI, it is difficult to determine whether a certain effect is of practical importance. For example, a difference in pointing time of 100 ms between two pointing techniques can be large or small depending on the application, how often it is used, its context of use, etc. In such cases, forcing artificial interpretations of practical importance can hurt transparency. In many cases, it is sufficient to present effect sizes in a clear manner and leave the judgment of practical importance to the reader.</p>
<p>Simple effect sizes are often a better choice, because they provide the information necessary for an expert in the area to use their judgment to assess the practical impact of an effect size. For example, a difference in reaction time of 100ms is above the threshold of human perception, and therefore likely of practical impact. A difference of 100ms in receiving a chat message in an asynchronous chat application is likely less impactful, as it is small compared to the amount of time a chat message is generally expected to take. A difference in pointing time of 100ms between two pointing techniques might be large or small depending on the application, how often it is used, the context of use, etc. Presenting simple effect sizes in a clear way—with units—allows the expert author to argue why the effect size may or may not have practical importance <em>and</em> allow the expert reader to make their own judgment.</p>
</div>
<div id="effectsize_faq_small_medium_large" class="section level3">
<h3><span class="header-section-number">2.1.7</span> What about Cohen’s small, medium, and large effect sizes?</h3>
<p>Conventional thresholds are sometimes used for standardized effect sizes like Cohen’s <em>d</em>, labeling them “small”, “medium”, or “large”. These thresholds are however largely arbitrary <span class="citation">(Cummings <a href="#ref-Cummings2011">2011</a>)</span>. They were originally proposed by Cohen based on human heights and intelligence quotients <span class="citation">(Cohen <a href="#ref-Cohen1977">1977</a>)</span>, but Cohen, in the very text where he first introduced them, noted that these thresholds may not be directly applicable to other domains:</p>
<blockquote>
<p>The terms “small”, “medium”, and “large” are relative, not only to each other, but to the area of behavioral science or even more particularly to the specific content and research method being employed in any given investigation… In the face of this relativity, there is a certain risk inherent in offering conventional operational definitions for these terms for use in power analysis in as diverse a field of inquiry as behavioral science. This risk is nevertheless accepted in the belief that more is to be gained than lost by supplying a common conventional frame of reference which is recommended for use only when no better basis for estimating the ES index is available. <span class="citation">(Cohen <a href="#ref-Cohen1977">1977</a>)</span></p>
</blockquote>
<p>Cohen recommended the use of these thresholds only when no better frame of reference for assessing practical importance was available. However, hindsight has demonstrated that if such thresholds are offered, they will be adopted as a convenience, often without much thought to how they apply to the domain at hand <span class="citation">(Baguley <a href="#ref-Baguley2004">2004</a>; Lenth <a href="#ref-Lenth2001">2001</a>)</span>; Lenth has called this usage “canned effect sizes” <span class="citation">(Lenth <a href="#ref-Lenth2001">2001</a>)</span>. Once adopted, these thresholds make reports more opaque, by standardizing away units of measurement and categorizing results into arbitrary classes. Like Cummings <span class="citation">(<a href="#ref-Cummings2011">2011</a>)</span>, we recommend against assessing the importance of effects by labeling them using Cohen’s thresholds.</p>

</div>
</div>
<div id="effectsize_exemplar_simple" class="section level2">
<h2><span class="header-section-number">2.2</span> Exemplar: Simple effect size</h2>
<p><mark> This section is in <em>alpha</em>. We welcome help and feedback at all levels! If you would like to contribute, please see <a href="https://github.com/transparentstats/guidelines/wiki/Contributing-to-the-Guidelines">Contributing to the Guidelines</a>. </mark></p>
<div id="libraries-needed-for-this-analysis" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Libraries needed for this analysis</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(forcats)    <span class="co"># for fct_...()</span>
<span class="kw">library</span>(broom)      <span class="co"># for tidy()</span>
<span class="kw">library</span>(ggstance)   <span class="co"># for geom_pointrangeh(), stat_summaryh()</span></code></pre></div>
</div>
<div id="data" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Data</h3>
<p>Imagine a between-subjects design, with completion time (in milliseconds) measured in two groups, <code>A</code> and <code>B</code>, with 20 subjects each.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12</span>)
n &lt;-<span class="st"> </span><span class="dv">20</span>
data &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">group =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="dt">each =</span> n),
  <span class="dt">completion_time_ms =</span> <span class="kw">c</span>(
    <span class="kw">rlnorm</span>(n, <span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">170</span>), <span class="dt">sdlog =</span> <span class="fl">0.3</span>),
    <span class="kw">rlnorm</span>(n, <span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">50</span>), <span class="dt">sdlog =</span> <span class="fl">0.4</span>)
  )
)</code></pre></div>
<p>We assume a log-normal model of completion times, which is a commonly-used model of completion time <span class="citation">(Sauro and Lewis <a href="#ref-Sauro2010">2010</a>)</span> and ensures completion times are all positive.</p>
<p>A good first step in any analysis is always to visualize the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_data &lt;-<span class="st">  </span><span class="co"># save for the teaser figure</span>
<span class="st">  </span>data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> completion_time_ms)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_dotplot</span>(<span class="dt">binwidth=</span><span class="dv">5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_summaryh</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">xintercept =</span> ..x..), <span class="dt">fun.x =</span> mean, <span class="dt">geom =</span> <span class="st">&quot;vline&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_grid</span>(group <span class="op">~</span><span class="st"> </span>., <span class="dt">switch=</span><span class="st">&quot;y&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Completion time (ms)&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Group&quot;</span>)
p_data</code></pre></div>
<p><img src="_main_files/figure-html/es-simple-data_plot-1.png" width="384" /></p>
<p>This plot shows all observed completion times in each group (black dots) along with the mean in each group (dashed red lines).</p>
</div>
<div id="calculating-simple-effect-size" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Calculating simple effect size</h3>
<p>Since we have meaningful units (milliseconds), we will use the <em>difference</em> in mean completion time as our effect size. Following <a href="effectsize.html#effectsize_faq_how_reporting">our recommendations on how to report effect size</a>, we also need to report the uncertainty around the sample effect size.</p>
<p>There are several possible approaches to estimating the uncertainty in the difference between the two groups. For simplicity, we show one possible approach in this exemplar, but we provide a non-exhaustive comparison of a few other approaches in the <a href="#appendix_effectsize_simple">effect size guideline appendix</a>.</p>
</div>
<div id="difference-in-means-with-students-t-confidence-interval" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Difference in means with Student’s t confidence interval</h3>
<p>While the response distributions are non-normal, the sampling distribution of the difference in means will still be defined on <span class="math inline">\((-\infty, +\infty)\)</span> and approximately symmetrical (per the central limit theorem), so we can compute a <em>Student’s t distribution confidence interval</em> for the difference in means.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">t_result &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">t.test</span>(completion_time_ms <span class="op">~</span><span class="st"> </span>group, <span class="dt">data =</span> data) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">tidy</span>()    <span class="co"># put result in tidy tabular format</span>
t_result</code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="right">estimate</th>
<th align="right">estimate1</th>
<th align="right">estimate2</th>
<th align="right">statistic</th>
<th align="right">p.value</th>
<th align="right">parameter</th>
<th align="right">conf.low</th>
<th align="right">conf.high</th>
<th align="left">method</th>
<th align="left">alternative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">103.6021</td>
<td align="right">159.0898</td>
<td align="right">55.48774</td>
<td align="right">9.388748</td>
<td align="right">0</td>
<td align="right">28.66574</td>
<td align="right">81.02211</td>
<td align="right">126.182</td>
<td align="left">Welch Two Sample t-test</td>
<td align="left">two.sided</td>
</tr>
</tbody>
</table>
</div>
<p>The <code>tidy()</code>ed output of the <code>t.test()</code> function includes an estimate of the mean difference in milliseconds (<code>estimate</code>) as well as the lower (<code>conf.low</code>) and upper (<code>conf.high</code>) bounds of the 95% confidence interval.</p>
</div>
<div id="reporting-simple-effect-size" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Reporting simple effect size</h3>
<p>Ideally, we would have space in our paper to report the effect size graphically:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_simple_effect_size &lt;-<span class="st">   </span><span class="co"># save for the teaser figure</span>
<span class="st">  </span>t_result <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="st">&quot;A - B&quot;</span>, <span class="dt">x =</span> estimate, <span class="dt">xmin =</span> conf.low, <span class="dt">xmax =</span> conf.high)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_pointrangeh</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">0</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Mean difference in completion time (ms) with 95% CI&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>)
p_simple_effect_size</code></pre></div>
<p><img src="_main_files/figure-html/es-simple-ci_plot-1.png" width="480" /></p>
<p>This graphical report includes all of the <a href="effectsize.html#effectsize_faq_how_reporting">elements of an effect size report that we recommend</a>:</p>
<ul>
<li>The direction of the difference (indicated by the label <code>A - B</code>)</li>
<li>The type of estimate reported (mean difference)</li>
<li>The type of uncertainty indicated (95% CI)</li>
<li>The units (ms)</li>
</ul>
<p>Space may not always permit the graphical report. While it can be less easy to interpret, an alternative is a textual report. <strong>Such a report should still include all of the four elements listed above.</strong> For example:</p>
<blockquote>
<p>Group <code>A</code> had a greater mean completion time than group <code>B</code> by 104 milliseconds (95% CI: [81, 126]).</p>
</blockquote>
</div>
<div id="interpreting-effect-size-same-result-different-domains-different-interpretations" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Interpreting effect size: same result, different domains = different interpretations</h3>
<p>Because simple effect sizes include units, we can use our expert judgment to interpret the report. Authors may wish to do so in order to put their result in context. Because the report above includes everything necessary for other experts to come to their own conclusion, providing our own interpretation does not prevent readers from applying their own judgment and coming to different conclusions.</p>
<p>To illustrate the effect of domain on interpreting effect size, we will imagine two different domains that might have led to the same result reported above, and write a different interpretation of the data for each.</p>
<div id="domain-1-physical-prototyping" class="section level4">
<h4><span class="header-section-number">2.2.6.1</span> Domain 1: Physical prototyping</h4>
<p>Imagine the above study was from the comparison of a novel physical user interface prototyping system (treatment <code>B</code>) to the previous state of the art (<code>A</code>), and the completion time referred to the time for feedback to be given to the user after they perform an input action. We might report the following interpretation of the results:</p>
<blockquote>
<p>Technique <code>B</code> offers a <strong>large</strong> improvement in feedback time (~81 – 126ms mean decrease), resulting in feedback times that tend to be less than the threshold of human perception (less than about 100ms). By contrast, the larger feedback times offered by technique <code>A</code> tended to be above that threshold, possibly degrading users’ experience of the prototypes built using that technique.</p>
</blockquote>
</div>
<div id="domain-2-chatbots" class="section level4">
<h4><span class="header-section-number">2.2.6.2</span> Domain 2: Chatbots</h4>
<p>Imagine the same quantitative results, now in the context of a natural language chat bot designed to answer users’ questions. Here, technique <code>A</code> will be the novel system, with improved natural language capabilities compared to the previous state-of-the-art technique, <code>B</code>. We might report the following interpretation of the results:</p>
<blockquote>
<p>While technique <code>A</code> takes longer to respond to chat messages (~81–126ms increase in mean response time), we believe this difference is acceptable in the context of an asynchronous chat interface in which users do not expect instantaneous responses. When weighed against the improved natural language capabilites of technique <code>A</code>, we believe this <strong>small</strong> increase in response time for messages is worth the improved message content.</p>
</blockquote>
<p>The same effect size is plausibly described as <strong>large</strong> in domain 1 and <strong>small</strong> in domain 2, illustrating the importance of expert interpretation to reporting and understanding effect size and the difficulty in applying pre-defined thresholds across domains.</p>

</div>
</div>
</div>
<div id="effectsize_exemplar_within" class="section level2">
<h2><span class="header-section-number">2.3</span> Exemplar: Within-subjects experiment</h2>
<p><mark> This section is in <em>alpha</em>. We welcome help and feedback at all levels! If you would like to contribute, please see <a href="https://github.com/transparentstats/guidelines/wiki/Contributing-to-the-Guidelines">Contributing to the Guidelines</a>. </mark></p>
<p>Large individual differences can be a major source of noise. An effective way of accounting for that noise is for every subject to run in every combination of conditions multiple times. This “<em>within-subject</em>” experiment design combined with many repetitions per condition can substantially reduce any noise from individual differences, allowing for more precise measurements despite a small number of subjects.</p>
<p>In this example, we’ll pretend we’ve run an experiment that compared different interfaces for visualizing data. Here are the parameters that we manipulate in the experiment:</p>
<ul>
<li>Independent Variable <strong>layout</strong>: the two layouts of the interface</li>
<li>Independent Variable <strong>size</strong>: the size of the dataset visualized (small, medium, and large)</li>
<li>Independent Variable <strong>color</strong>: interface color, where we don’t expect any effect</li>
</ul>
<p>We run each subject through each combination of these variables 20 times to get (2 layouts) × (3 sizes) × (4 colors) × (20 repetitions) = 480 trials per subject. We measure some reponse (e.g., reponse time) in each trial.</p>
<div id="libraries-needed-for-this-analysis-1" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Libraries needed for this analysis</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(afex)       <span class="co"># for aov_ez()</span>
<span class="kw">library</span>(parallel)   <span class="co"># for parLapply()</span></code></pre></div>
</div>
<div id="simulate-a-dataset" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Simulate a dataset</h3>
<div id="subjects-conditions-and-repetitions" class="section level4">
<h4><span class="header-section-number">2.3.2.1</span> Subjects, conditions, and repetitions</h4>
<p>In this example, there are 6 subjects (<code>subject</code> column).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">543</span>) <span class="co"># make the output consistent</span>
SUBJECT_COUNT =<span class="st"> </span><span class="dv">6</span>

data &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(
  <span class="dt">subject =</span> <span class="kw">paste</span>(<span class="st">&#39;Subject&#39;</span>, LETTERS[<span class="dv">1</span><span class="op">:</span>SUBJECT_COUNT]), <span class="co"># subject IDs</span>
  <span class="dt">layout =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="co"># independent variable</span>
  <span class="dt">size =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>, <span class="co"># independent variable</span>
  <span class="dt">color =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">3</span>, <span class="co"># independent variable</span>
  <span class="dt">repetition =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">20</span> <span class="co"># each subject runs in each condition multiple times</span>
)</code></pre></div>
</div>
<div id="individual-differences" class="section level4">
<h4><span class="header-section-number">2.3.2.2</span> Individual differences</h4>
<p>Not all subjects behave the same way. Some people might be tired, bad at the task, or just not trying very hard. These performance differences can’t be directly measured, but they can substantially impact the results. We’ll simulate these individual differences by giving each subject a random performance handicap.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># build a table of subject performance multipliers</span>
individualDifferences &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">subject =</span> <span class="kw">unique</span>(data<span class="op">$</span>subject))
individualDifferences<span class="op">$</span>handicap &lt;-<span class="st"> </span><span class="kw">rnorm</span>(SUBJECT_COUNT, <span class="dv">20</span>, <span class="dv">4</span>) <span class="co"># individual differences</span>

<span class="co"># put it in the main dataset</span>
data &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">left_join</span>(individualDifferences, <span class="dt">by =</span> <span class="st">&quot;subject&quot;</span>)</code></pre></div>
</div>
<div id="simulate-some-noisy-effects" class="section level4">
<h4><span class="header-section-number">2.3.2.3</span> Simulate some noisy effects</h4>
<p>We’ll simulate an experiment with a main effect of <code>layout</code> and <code>size</code> and an interaction between them. However, <code>color</code> and its interactions will not have an impact.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate the response times with a clean model</span>
data &lt;-<span class="st"> </span>
<span class="st">  </span>data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
  <span class="dt">response_time =</span> 
    layout <span class="op">*</span><span class="st"> </span>.<span class="dv">4</span> <span class="op">+</span><span class="st"> </span><span class="co"># main effect of layout</span>
<span class="st">    </span>size <span class="op">*</span><span class="st"> </span>.<span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="co"># main effect of size</span>
<span class="st">    </span>color <span class="op">*</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>layout <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>.<span class="dv">6</span> <span class="op">+</span><span class="st"> </span><span class="co"># 2-way interaction</span>
<span class="st">    </span>size <span class="op">*</span><span class="st"> </span>color <span class="op">*</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>layout <span class="op">*</span><span class="st"> </span>color <span class="op">*</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>layout <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>color <span class="op">*</span><span class="st"> </span><span class="dv">0</span>
)</code></pre></div>
<pre><code>## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># add some reponse noise</span>
data &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">response_time =</span> response_time <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">n</span>()))

<span class="co"># add noise from individual handicaps</span>
data &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">response_time =</span> <span class="dv">30</span> <span class="op">+</span><span class="st"> </span>handicap<span class="op">*</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>response_time <span class="op">*</span><span class="st"> </span>handicap)</code></pre></div>
<p>Even though we used numbers to simulate the model, the independent variables and subject ID are all factors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span>
<span class="st">  </span>data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">subject =</span> <span class="kw">factor</span>(subject), 
    <span class="dt">layout =</span> <span class="kw">factor</span>(layout), 
    <span class="dt">color =</span> <span class="kw">factor</span>(color)
  )</code></pre></div>
</div>
</div>
<div id="a-look-at-the-data" class="section level3">
<h3><span class="header-section-number">2.3.3</span> A look at the data</h3>
<p>Let’s get an overview of the results by graphing each subject’s average response time for each condition.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(layout, size, color, subject) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">response_time =</span> <span class="kw">mean</span>(response_time)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">aes</span>(<span class="dt">y=</span>response_time, <span class="dt">x=</span>size, <span class="dt">linetype=</span>layout, <span class="dt">color=</span>color, 
      <span class="dt">group=</span><span class="kw">paste</span>(layout,color,subject)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="fl">1.5</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_color_brewer</span>(<span class="dt">palette=</span><span class="st">&#39;Set1&#39;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>subject) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title=</span><span class="st">&#39;Response times for each subject&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="_main_files/figure-html/within-sneak-peak-1.png" width="672" /></p>
<p>Despite a lot of variability in raw values between subjects (individual differences), we can see some consistent patterns. The dashed lines are higher (main effect) and more sloped (interaction) than the solid lines. But there doesn’t seem to be any consistent ordering of the colors.</p>
</div>
<div id="compute-effect-sizes" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Compute effect sizes</h3>
<p>While <strong>Cohen’s <em>d</em> </strong> is often used for simple 2-factor, single-trial, between-subject designs, repetition skews the measure to be very high. Experiment results with lots of repetition can be more reliably interpretted with the <strong>eta squared (<span class="math inline">\(\eta^{2}\)</span>)</strong> family of effect sizes, which represent the proportion of variance accounted for by a particular variable. A variant, <strong>generalized eta squared (<span class="math inline">\(\eta_{G}^{2}\)</span>)</strong>, is particularly suited for providing comparable effect sizes in both between and within-subject designs <span class="citation">(Olejnik and Algina <a href="#ref-Olejnik2003">2003</a>; Bakeman <a href="#ref-Bakeman2005">2005</a>)</span>. This property makes it more easily applicable to meta-analyses.</p>
<p>For those accustomed to Cohen’s <em>d</em>, it’s important to be aware that <span class="math inline">\(\eta_{G}^{2}\)</span> is typically smaller, with a Cohen’s d of 0.2 being equivalent to a <span class="math inline">\(\eta_{G}^{2}\)</span> of around 0.02. Also, the actual number has little meaning beyond its scale relative to other effects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">results =<span class="st"> </span>afex<span class="op">::</span><span class="kw">aov_ez</span>(
  <span class="dt">data =</span> data, 
  <span class="dt">id =</span> <span class="st">&#39;subject&#39;</span>, <span class="co"># subject id column</span>
  <span class="dt">dv =</span> <span class="st">&#39;response_time&#39;</span>, <span class="co"># dependent variable</span>
  <span class="dt">within =</span> <span class="kw">c</span>(<span class="st">&#39;layout&#39;</span>, <span class="st">&#39;size&#39;</span>, <span class="st">&#39;color&#39;</span>), <span class="co"># within-subject independent variables</span>
  <span class="dt">between =</span> <span class="ot">NULL</span> ,<span class="co"># between-subject independent variables</span>
  <span class="dt">fun_aggregate =</span> mean, <span class="co"># average multiple repetitions together for each subject*condition</span>
  <span class="dt">anova_table =</span> <span class="kw">list</span>(<span class="dt">es =</span> <span class="st">&#39;ges&#39;</span>) <span class="co"># effect size = generalized eta squared</span>
)</code></pre></div>
<p><em>Note: <code>fun_aggregate = mean</code> collapses repetitions into a mean, which may be a problem if an experiment is not fully counterbalanced. This example, however, has every subject running in every combination of conditions, so simple collapsing is the correct procedure.</em></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">anova_results &lt;-<span class="st"> </span>
<span class="st">  </span>results<span class="op">$</span>anova_table <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">rownames_to_column</span>(<span class="st">&#39;effect&#39;</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># put effect names in a column</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st">`</span><span class="dt">Pr(&gt;F)</span><span class="st">`</span>) <span class="co"># no need to show p-values</span>
  
anova_results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.tibble</span>()</code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left">effect</th>
<th align="right">num Df</th>
<th align="right">den Df</th>
<th align="right">MSE</th>
<th align="right">F</th>
<th align="right">ges</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">layout</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">86.61117</td>
<td align="right">186.9153824</td>
<td align="right">0.5489428</td>
</tr>
<tr class="even">
<td align="left">size</td>
<td align="right">2</td>
<td align="right">10</td>
<td align="right">51.97262</td>
<td align="right">108.2961322</td>
<td align="right">0.4583589</td>
</tr>
<tr class="odd">
<td align="left">color</td>
<td align="right">3</td>
<td align="right">15</td>
<td align="right">14.50618</td>
<td align="right">0.9125176</td>
<td align="right">0.0029764</td>
</tr>
<tr class="even">
<td align="left">layout:size</td>
<td align="right">2</td>
<td align="right">10</td>
<td align="right">31.84102</td>
<td align="right">59.8437233</td>
<td align="right">0.2226922</td>
</tr>
<tr class="odd">
<td align="left">layout:color</td>
<td align="right">3</td>
<td align="right">15</td>
<td align="right">19.24758</td>
<td align="right">0.1492400</td>
<td align="right">0.0006474</td>
</tr>
<tr class="even">
<td align="left">size:color</td>
<td align="right">6</td>
<td align="right">30</td>
<td align="right">21.02831</td>
<td align="right">0.4695047</td>
<td align="right">0.0044335</td>
</tr>
<tr class="odd">
<td align="left">layout:size:color</td>
<td align="right">6</td>
<td align="right">30</td>
<td align="right">20.73981</td>
<td align="right">1.4065459</td>
<td align="right">0.0129870</td>
</tr>
</tbody>
</table>
</div>
<p>Looking at the <code>F</code> and <code>ges</code> (generalized eta squared) columns, <code>layout</code> and <code>size</code> and the interaction between <code>layout</code> and <code>size</code> account for much more of the noise than <code>color</code> and the other 2-way and 3-way interactions do.</p>
</div>
<div id="bootstrapping" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Bootstrapping</h3>
<p><mark> Draft. Needs work. </mark></p>
<p>But that only gives us one one point estimate per effect, whereas we want a confidence interval.</p>
<p>We’ll use a technique called bootstrapping, which checks the effect size for many randomized samples of the data. Importantly, bootstrapping samples “with replacement”, meaning that items can be sampled more than once or not at all. If a small subset of the observations are driving an effect, they won’t impact all of the samples. Consequently, the spread of the bootstrapped confidence intervals shows how consistent the results are for different samples.</p>
<ol style="list-style-type: decimal">
<li>Randomly sample with replacement (meaning the same value might be sampled more than once) to build a new dataset</li>
<li>Perform the analysis on this new dataset</li>
<li>Do that many times</li>
<li>Sort the results for each effect and find the 95% confidence interval</li>
</ol>
<div id="prepare-for-bootstrapping" class="section level4">
<h4><span class="header-section-number">2.3.5.1</span> Prepare for bootstrapping</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data used for bootstrapping will collapse by each subject-condition combination</span>
data_aggregated &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(layout, size, color, subject) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">response_time =</span> <span class="kw">mean</span>(response_time))</code></pre></div>
</div>
<div id="each-iteration" class="section level4">
<h4><span class="header-section-number">2.3.5.2</span> Each iteration</h4>
<p>Each iteration of the bootstrap samples the original dataset and runs the analysis on this permutation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># run one iteration of the bootstrap procedure</span>
analyze_one_iteration &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  subjects &lt;-<span class="st"> </span><span class="kw">unique</span>(data_aggregated<span class="op">$</span>subject)
  
  <span class="co"># select subjects at random with replacement</span>
  sampled_subjects &lt;-<span class="st"> </span><span class="kw">sample</span>(subjects, <span class="kw">length</span>(subjects), <span class="dt">replace=</span><span class="ot">TRUE</span>)
  
  <span class="co"># get all the results for one subject</span>
  <span class="co"># and give them a new unique subject id</span>
  get_one_subjects_data &lt;-<span class="st"> </span><span class="cf">function</span>(i) {
    data_aggregated <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">filter</span>(subject <span class="op">==</span><span class="st"> </span>sampled_subjects[i]) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">mutate</span>(<span class="dt">subject =</span> <span class="kw">paste</span>(sampled_subjects[i], i))
  }
  
  <span class="co"># get all of the boostrapped subjects&#39; data</span>
  boot_data &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(sampled_subjects), get_one_subjects_data) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">bind_rows</span>()
  
  <span class="co"># compute the effect sizes the same way we did without bootstrapping</span>
  afex<span class="op">::</span><span class="kw">aov_ez</span>(
    <span class="dt">data =</span> boot_data, 
    <span class="dt">id =</span> <span class="st">&#39;subject&#39;</span>, <span class="co"># subject id column</span>
    <span class="dt">dv =</span> <span class="st">&#39;response_time&#39;</span>, <span class="co"># dependent variable</span>
    <span class="dt">within =</span> <span class="kw">c</span>(<span class="st">&#39;layout&#39;</span>, <span class="st">&#39;size&#39;</span>, <span class="st">&#39;color&#39;</span>), <span class="co"># within-subject independent variables</span>
    <span class="dt">between =</span> <span class="ot">NULL</span> ,<span class="co"># between-subject independent variables</span>
    <span class="co">#fun_aggregate = mean,</span>
    <span class="dt">anova_table =</span> <span class="kw">list</span>(<span class="dt">es =</span> <span class="st">&#39;ges&#39;</span>) <span class="co"># effect size = generalized eta squared</span>
  )<span class="op">$</span>anova_table <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">as.tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">rownames_to_column</span>(<span class="st">&#39;effect&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># put effect names in a column</span>
<span class="st">    </span><span class="kw">return</span>()
}</code></pre></div>
</div>
<div id="iterate" class="section level4">
<h4><span class="header-section-number">2.3.5.3</span> Iterate</h4>
<p>The bootstrap needs to run many times to determine how a subset of the data impacts</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># run many iterations of the bootstrap procedure</span>
analyze_many_iterations =<span class="st"> </span><span class="cf">function</span> (bootstrap_iteration_count) {
  <span class="co"># each core needs to reload the libraries</span>
  <span class="kw">library</span>(tidyverse)
  <span class="kw">library</span>(afex)
  <span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span>bootstrap_iteration_count, <span class="cf">function</span>(x) <span class="kw">analyze_one_iteration</span>(x)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">bind_rows</span>()
}</code></pre></div>
</div>
<div id="parallelize" class="section level4">
<h4><span class="header-section-number">2.3.5.4</span> Parallelize</h4>
<p>Bootstrapping can be slow, especially with thousands of iterations. Splitting the iterations across processor cores cuts down on the wait time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">BOOTSTRAP_COUNT &lt;-<span class="st"> </span><span class="dv">100</span> <span class="co"># at least 5k recommended. Use lower values for quicker testing.</span>

<span class="co"># Initiate cluster</span>
core_count &lt;-<span class="st"> </span><span class="kw">detectCores</span>() <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
core_count &lt;-<span class="st"> </span><span class="kw">ifelse</span>(core_count <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>, <span class="dv">1</span>, core_count)
my_cluster &lt;-<span class="st"> </span><span class="kw">makeCluster</span>(core_count)
<span class="co"># make sure each core in the cluster defines these functions</span>
<span class="kw">clusterExport</span>(my_cluster, <span class="st">&quot;analyze_one_iteration&quot;</span>)
<span class="kw">clusterExport</span>(my_cluster, <span class="st">&quot;data_aggregated&quot;</span>)
<span class="co"># how many times should each core iterate </span>
bootstrap_iteration_count &lt;-<span class="st"> </span>BOOTSTRAP_COUNT <span class="op">/</span><span class="st"> </span>core_count
<span class="co"># run the bootstrap and output the time</span>
<span class="kw">system.time</span>(
  boot_results &lt;-<span class="st"> </span><span class="kw">parLapply</span>(
    my_cluster, <span class="co"># the cluster of cores</span>
    <span class="kw">rep</span>(bootstrap_iteration_count, core_count), <span class="co"># how many runs for each core</span>
    analyze_many_iterations) <span class="co"># the function to run in each core</span>
)</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.003   0.000  11.117</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the cluster is no longer needed</span>
<span class="kw">stopCluster</span>(my_cluster)
<span class="co"># cleanup</span>
<span class="kw">rm</span>(core_count, my_cluster, bootstrap_iteration_count, data_aggregated)</code></pre></div>
</div>
</div>
<div id="getting-a-confidence-interval-from-a-bootstrap" class="section level3">
<h3><span class="header-section-number">2.3.6</span> Getting a confidence interval from a bootstrap</h3>
<p>Each bootstrap iterations ran one anaylsis, so wwe now have many results. So for each effect size, we sort the results and find the range of the inner 95%.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># inner 95%</span>
PERCENTILE_LO &lt;-<span class="st"> </span><span class="fl">0.025</span>
PERCENTILE_HI &lt;-<span class="st"> </span><span class="fl">0.975</span>

<span class="co"># put all the boostraped results together</span>
boot_results &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(boot_results)
boot_results &lt;-<span class="st"> </span>
<span class="st">  </span>boot_results <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(effect) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(
    <span class="dt">effectsize_conf_low =</span> <span class="kw">unname</span>(<span class="kw">quantile</span>(ges, <span class="dt">probs =</span> PERCENTILE_LO)),
    <span class="dt">effectsize_conf_high =</span> <span class="kw">unname</span>(<span class="kw">quantile</span>(ges, <span class="dt">probs =</span> PERCENTILE_HI)))

<span class="co"># add the low and hi end estimates to the effect size table</span>
anova_results &lt;-<span class="st"> </span>
<span class="st">  </span>anova_results <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">left_join</span>(boot_results, <span class="dt">by =</span> <span class="st">&#39;effect&#39;</span>)
<span class="co"># show the table</span>
anova_results <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.tibble</span>()</code></pre></div>
<div class="kable-table">
<table>
<thead>
<tr class="header">
<th align="left">effect</th>
<th align="right">num Df</th>
<th align="right">den Df</th>
<th align="right">MSE</th>
<th align="right">F</th>
<th align="right">ges</th>
<th align="right">effectsize_conf_low</th>
<th align="right">effectsize_conf_high</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">layout</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">86.61117</td>
<td align="right">186.9153824</td>
<td align="right">0.5489428</td>
<td align="right">0.3771272</td>
<td align="right">0.8402746</td>
</tr>
<tr class="even">
<td align="left">size</td>
<td align="right">2</td>
<td align="right">10</td>
<td align="right">51.97262</td>
<td align="right">108.2961322</td>
<td align="right">0.4583589</td>
<td align="right">0.3194852</td>
<td align="right">0.7699754</td>
</tr>
<tr class="odd">
<td align="left">color</td>
<td align="right">3</td>
<td align="right">15</td>
<td align="right">14.50618</td>
<td align="right">0.9125176</td>
<td align="right">0.0029764</td>
<td align="right">0.0013143</td>
<td align="right">0.0450325</td>
</tr>
<tr class="even">
<td align="left">layout:size</td>
<td align="right">2</td>
<td align="right">10</td>
<td align="right">31.84102</td>
<td align="right">59.8437233</td>
<td align="right">0.2226922</td>
<td align="right">0.1223908</td>
<td align="right">0.5574384</td>
</tr>
<tr class="odd">
<td align="left">layout:color</td>
<td align="right">3</td>
<td align="right">15</td>
<td align="right">19.24758</td>
<td align="right">0.1492400</td>
<td align="right">0.0006474</td>
<td align="right">0.0002076</td>
<td align="right">0.0301530</td>
</tr>
<tr class="even">
<td align="left">size:color</td>
<td align="right">6</td>
<td align="right">30</td>
<td align="right">21.02831</td>
<td align="right">0.4695047</td>
<td align="right">0.0044335</td>
<td align="right">0.0022358</td>
<td align="right">0.0609152</td>
</tr>
<tr class="odd">
<td align="left">layout:size:color</td>
<td align="right">6</td>
<td align="right">30</td>
<td align="right">20.73981</td>
<td align="right">1.4065459</td>
<td align="right">0.0129870</td>
<td align="right">0.0063141</td>
<td align="right">0.0938595</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="plot-the-effect-sizes" class="section level3">
<h3><span class="header-section-number">2.3.7</span> Plot the effect sizes</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">anova_results <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># reverse order the factors so that they appear in proper order in the plot</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">effect =</span> <span class="kw">fct_rev</span>(<span class="kw">fct_inorder</span>(effect))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>
<span class="st">  </span><span class="co"># plot and mapping</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> effect, <span class="dt">y =</span> ges, <span class="dt">ymin =</span> effectsize_conf_low, <span class="dt">ymax =</span> effectsize_conf_high)) <span class="op">+</span>
<span class="st">    </span><span class="co"># reference line of no-effect</span>
<span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">linetype =</span> <span class="st">&#39;dotted&#39;</span>) <span class="op">+</span>
<span class="st">    </span><span class="co"># point- and interval-estimates</span>
<span class="st">    </span><span class="kw">geom_pointrange</span>() <span class="op">+</span>
<span class="st">    </span><span class="co"># ensures that we see the reference line</span>
<span class="st">    </span><span class="kw">expand_limits</span>(<span class="dt">x =</span> <span class="dv">0</span>) <span class="op">+</span>
<span class="st">    </span><span class="co"># labels</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&#39;Effect size &#39;</span>, eta[G]<span class="op">^</span><span class="dv">2</span>))) <span class="op">+</span>
<span class="st">    </span><span class="co"># flip to horizontal plot and apply black-and-white theme</span>
<span class="st">    </span><span class="kw">coord_flip</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="_main_files/figure-html/within-boostrap-4-1.png" width="672" /></p>
</div>
<div id="reporting-the-results" class="section level3">
<h3><span class="header-section-number">2.3.8</span> Reporting the results</h3>
<p>Generalized eta squared (GES) represents the proportion of variance in the results explained by each variable. The previous graph shows clear main effects for <code>layout</code> and <code>size</code> and an interaction between <code>layout</code> and <code>size</code>. However <code>color</code> and the other 2-way and 3-way interactions are relatively much smaller, barely above zero. There is no useful cutoff for what counts as a “significant” effect, so think in terms of relative size – which variables best explain the variance in the results?</p>
<p>Strong effects:</p>
<ul>
<li><strong>layout:</strong> F<sub>1,5</sub> = 187, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.549 95% CI [0.377, 0.84]</li>
<li><strong>size:</strong> F<sub>2,10</sub> = 108, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.458 95% CI [0.319, 0.77]</li>
<li><strong>layout</strong> × <strong>size:</strong> F<sub>2,10</sub> = 59.8, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.223 95% CI [0.122, 0.557]</li>
</ul>
<p>Minimally impactful:</p>
<ul>
<li><strong>color</strong> F<sub>3,15</sub> = 0.913, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.00298 95% CI [0.00131, 0.045]</li>
<li><strong>layout</strong> × <strong>color:</strong> F<sub>3,15</sub> = 0.149, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.000647 95% CI [0.000208, 0.0302]</li>
<li><strong>size</strong> × <strong>color:</strong> F<sub>6,30</sub> = 0.47, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.00443 95% CI [0.00224, 0.0609]</li>
<li><strong>layout</strong> × <strong>size</strong> × <strong>color:</strong> F<sub>6,30</sub> = 1.41, <span class="math inline">\(\eta_{G}^{2}\)</span> = 0.013 95% CI [0.00631, 0.0939]</li>
</ul>

</div>
</div>
<div id="effectsize_exemplar_standardized" class="section level2">
<h2><span class="header-section-number">2.4</span> Exemplar: Standardized effect size</h2>
<p><mark> This section is in <em>alpha</em>. We welcome help and feedback at all levels! If you would like to contribute, please see <a href="https://github.com/transparentstats/guidelines/wiki/Contributing-to-the-Guidelines">Contributing to the Guidelines</a>. </mark></p>
<pre><code>TODO: This needs a domain where we can argue that Cohen&#39;s d is an exemplar analysis, then repeat structure of exemplar 1 with it
May be an example of existing meta-analysis in HCI.</code></pre>
<div id="libraries-needed-for-this-analysis-2" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Libraries needed for this analysis</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(effsize)  <span class="co"># for cohen.d()</span></code></pre></div>
</div>
<div id="standardized-effect-size" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Standardized effect size</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12</span>)
n &lt;-<span class="st"> </span><span class="dv">20</span>
data &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">group =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="dt">each =</span> n),
  <span class="dt">completion_time_ms =</span> <span class="kw">c</span>(
    <span class="kw">rlnorm</span>(n, <span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">170</span>), <span class="dt">sdlog =</span> <span class="fl">0.3</span>),
    <span class="kw">rlnorm</span>(n, <span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">50</span>), <span class="dt">sdlog =</span> <span class="fl">0.4</span>)
  )
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cohen_d &lt;-<span class="st"> </span><span class="kw">cohen.d</span>(completion_time_ms <span class="op">~</span><span class="st"> </span>group, <span class="dt">data =</span> data)

<span class="co"># manual calculation</span>
data_A &lt;-<span class="st"> </span>(data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(group <span class="op">==</span><span class="st"> &quot;A&quot;</span>))[[<span class="st">&quot;completion_time_ms&quot;</span>]]
data_B &lt;-<span class="st"> </span>(data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(group <span class="op">==</span><span class="st"> &quot;B&quot;</span>))[[<span class="st">&quot;completion_time_ms&quot;</span>]]
sd_A &lt;-<span class="st"> </span><span class="kw">sd</span>(data_A)
sd_B &lt;-<span class="st"> </span><span class="kw">sd</span>(data_B)
sd_pool &lt;-<span class="st"> </span><span class="kw">sqrt</span>( (sd_A<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>sd_B<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> )
cohen_d_manual &lt;-<span class="st"> </span><span class="kw">abs</span>(<span class="kw">mean</span>(data_A) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(data_B))<span class="op">/</span>sd_pool</code></pre></div>
<p><strong>Standardized effect size:</strong> Cohen’s d = 2.97 SDs with 95% confidence interval [2.04 , 3.90]</p>

</div>
</div>
<div id="effectsize_exemplar_nonparametric" class="section level2">
<h2><span class="header-section-number">2.5</span> Exemplar: Nonparametric effect size</h2>
<p><mark> This section is in <em>alpha</em>. We welcome help and feedback at all levels! If you would like to contribute, please see <a href="https://github.com/transparentstats/guidelines/wiki/Contributing-to-the-Guidelines">Contributing to the Guidelines</a>. </mark></p>
<p>For a nonparametric test that produces a Z-score, like the Mann-Whitney U test or the Wilcoxon Signed-Rank test, an effect size can be computed as:</p>
<center>
<span class="math inline">\(r = \left|\frac{Z}{\sqrt{N}}\right|\)</span>
</center>
<p>Above, Z is the Z-score and N is the number of observations in all groups [<span class="citation">Rosenthal (<a href="#ref-Rosenthal1991a">1991</a>)</span>, p. 19). The result, <em>r</em>, is a variance-based effect size, like Pearson <em>r</em>, not a Cohen <em>d</em>-family effect size. The <em>r</em> can be squared to estimate the percentage of variance explained, however it will not be exactly equivalent to the Pearson <em>r</em>.</p>
<pre><code>TODO: This needs a domain where we can argue that the nonparametric approach is an exemplar analysis, then repeat structure of exemplar 1 with it</code></pre>
<div id="libraries-needed-for-this-analysis-3" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Libraries needed for this analysis</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(coin)        <span class="co"># for wilcox_test</span></code></pre></div>
</div>
<div id="nonparametric-effect-size" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Nonparametric effect size</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12</span>)
n &lt;-<span class="st"> </span><span class="dv">20</span>
data &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">group =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="dt">each =</span> n),
  <span class="dt">completion_time_ms =</span> <span class="kw">c</span>(
    <span class="kw">rlnorm</span>(n, <span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">170</span>), <span class="dt">sdlog =</span> <span class="fl">0.3</span>),
    <span class="kw">rlnorm</span>(n, <span class="dt">meanlog =</span> <span class="kw">log</span>(<span class="dv">50</span>), <span class="dt">sdlog =</span> <span class="fl">0.4</span>)
  )
)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_A &lt;-<span class="st"> </span>(data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(group <span class="op">==</span><span class="st"> &quot;A&quot;</span>))[[<span class="st">&quot;completion_time_ms&quot;</span>]]
data_B &lt;-<span class="st"> </span>(data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(group <span class="op">==</span><span class="st"> &quot;B&quot;</span>))[[<span class="st">&quot;completion_time_ms&quot;</span>]]
wilcox_result &lt;-<span class="st"> </span><span class="kw">wilcox_test</span>(completion_time_ms <span class="op">~</span><span class="st"> </span><span class="kw">factor</span>(group), <span class="dt">data =</span> data)
effect_r &lt;-<span class="st"> </span><span class="kw">abs</span>(wilcox_result<span class="op">@</span>statistic<span class="op">@</span>teststatistic <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">nrow</span>(data)))</code></pre></div>
<p><strong>Non-parametric effect size:</strong> Variance-based effect size <em>r</em> = 0.84.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Cumming2013a">
<p>Cumming, Geoff. 2013. <em>Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis</em>. Routledge.</p>
</div>
<div id="ref-Wilkinson1999a">
<p>Wilkinson, Leland. 1999. “Statistical Methods in Psychology Journals: Guidelines and Explanations.” <em>American Psychologist</em> 54 (8). American Psychological Association: 594.</p>
</div>
<div id="ref-Cumming2014a">
<p>Cumming, Geoff. 2014. “The New Statistics: Why and How.” <em>Psychological Science</em> 25 (1): 7–29. doi:<a href="https://doi.org/10.1177/0956797613504966">10.1177/0956797613504966</a>.</p>
</div>
<div id="ref-APA2001">
<p>American Psychological Association. 2001. <em>Publication Manual of the American Psychological Association</em>. 5th Editio.</p>
</div>
<div id="ref-Loftus1993">
<p>Loftus, Geoffrey R. 1993. “A Picture Is Worth a Thousandp Values: On the Irrelevance of Hypothesis Testing in the Microcomputer Age.” <em>Behavior Research Methods, Instruments, &amp; Computers</em> 25 (2). Springer: 250–56. <a href="https://faculty.washington.edu/gloftus/Research/Publications/Manuscript.pdf/Loftus%20p-values%201993.pdf">https://faculty.washington.edu/gloftus/Research/Publications/Manuscript.pdf/Loftus%20p-values%201993.pdf</a>.</p>
</div>
<div id="ref-Kastellec2007">
<p>Kastellec, Jonathan P, and Eduardo L Leoni. 2007. “Using Graphs Instead of Tables in Political Science.” <em>Perspectives on Politics</em> 5 (04). Cambridge Univ Press: 755–71.</p>
</div>
<div id="ref-Cohen1988a">
<p>Cohen, Jacob. 1988. <em>Statistical Power Analysis for the Behavioral Sciences</em>. Lawrence Earlbaum Associates.</p>
</div>
<div id="ref-Cummings2011">
<p>Cummings, Peter. 2011. “Arguments for and Against Standardized Mean Differences (Effect Sizes).” <em>Archives of Pediatrics &amp; Adolescent Medicine</em> 165 (7): 592. doi:<a href="https://doi.org/10.1001/archpediatrics.2011.97">10.1001/archpediatrics.2011.97</a>.</p>
</div>
<div id="ref-Baguley2009">
<p>Baguley, Thom. 2009. “Standardized or simple effect size: what should be reported?” <em>British Journal of Psychology</em> 100 (3): 603–17. doi:<a href="https://doi.org/10.1348/000712608X377117">10.1348/000712608X377117</a>.</p>
</div>
<div id="ref-Cohen1977">
<p>Cohen, Jacob. 1977. “The t Test for Means.” In <em>Statistical Power Analysis for the Behavioral Sciences</em>, Revised Ed, 19–74. Academic Press. doi:<a href="https://doi.org/10.1016/B978-0-12-179060-8.50007-4">10.1016/B978-0-12-179060-8.50007-4</a>.</p>
</div>
<div id="ref-Baguley2004">
<p>Baguley, Thom. 2004. “Understanding statistical power in the context of applied research.” <em>Applied Ergonomics</em> 35 (2): 73–80. doi:<a href="https://doi.org/10.1016/j.apergo.2004.01.002">10.1016/j.apergo.2004.01.002</a>.</p>
</div>
<div id="ref-Lenth2001">
<p>Lenth, Russel V. 2001. “Some practical guidelines for effective sample size determination.” <em>The American Statistician</em> 55 (3): 187–93. doi:<a href="https://doi.org/10.1198/000313001317098149">10.1198/000313001317098149</a>.</p>
</div>
<div id="ref-Sauro2010">
<p>Sauro, Jeff, and James R. Lewis. 2010. “Average task times in usability tests.” <em>Proceedings of the 28th International Conference on Human Factors in Computing Systems - CHI ’10</em>. doi:<a href="https://doi.org/10.1145/1753326.1753679">10.1145/1753326.1753679</a>.</p>
</div>
<div id="ref-Olejnik2003">
<p>Olejnik, Stephen, and James Algina. 2003. “Generalized Eta and Omega Squared Statistics: Measures of Effect Size for Some Common Research Designs.” <em>Psychological Methods</em>.</p>
</div>
<div id="ref-Bakeman2005">
<p>Bakeman, Roger. 2005. “Recommended Effect Size Statistics for Repeated Measures Designs.” <em>Behavior Research Methods</em>.</p>
</div>
<div id="ref-Rosenthal1991a">
<p>Rosenthal, Robert. 1991. <em>Meta-Analytic Procedures for Social Research</em>. Vol. 6. Sage.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="transparent-statistics-guiding-principles.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/transparentstats/guidelines/edit/master/guides/effectsize.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
