<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Transparent Statistics Guiding Principles | Transparent Statistics Guidelines</title>
  <meta name="description" content="Guidelines, FAQ, and exemplar analyses" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Transparent Statistics Guiding Principles | Transparent Statistics Guidelines" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Guidelines, FAQ, and exemplar analyses" />
  <meta name="github-repo" content="transparentstats/guidelines" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Transparent Statistics Guiding Principles | Transparent Statistics Guidelines" />
  
  <meta name="twitter:description" content="Guidelines, FAQ, and exemplar analyses" />
  

<meta name="author" content="Transparent Statistics in HCI Working Group (http://transparentstatistics.org/)" />


<meta name="date" content="2019-06-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html">
<link rel="next" href="effectsize.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Transparent Statistics Guidelines</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#organization"><i class="fa fa-check"></i>Organization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#using"><i class="fa fa-check"></i>Using</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#limitations"><i class="fa fa-check"></i>Limitations</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#versions"><i class="fa fa-check"></i>Versions</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#citing"><i class="fa fa-check"></i>Citing</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributing"><i class="fa fa-check"></i>Contributing</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="principles.html"><a href="principles.html"><i class="fa fa-check"></i><b>1</b> Transparent Statistics Guiding Principles</a><ul>
<li class="chapter" data-level="1.1" data-path="principles.html"><a href="principles.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="principles.html"><a href="principles.html#guiding-principles"><i class="fa fa-check"></i><b>1.2</b> Guiding Principles</a><ul>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#faithfulness"><i class="fa fa-check"></i>1. Faithfulness</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#robustness"><i class="fa fa-check"></i>2. Robustness</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#resilience"><i class="fa fa-check"></i>3. Resilience</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#process-transparency"><i class="fa fa-check"></i>4. Process Transparency</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#clarity"><i class="fa fa-check"></i>5. Clarity</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#principles_simplicity"><i class="fa fa-check"></i>6. Simplicity</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#non-contingency"><i class="fa fa-check"></i>7. Non-contingency</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#precision-and-economy"><i class="fa fa-check"></i>8. Precision and economy</a></li>
<li class="chapter" data-level="" data-path="principles.html"><a href="principles.html#material-availability"><i class="fa fa-check"></i>9. Material availability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="effectsize.html"><a href="effectsize.html"><i class="fa fa-check"></i><b>2</b> Effect size</a><ul>
<li class="chapter" data-level="2.1" data-path="effectsize.html"><a href="effectsize.html#faq"><i class="fa fa-check"></i><b>2.1</b> FAQ</a><ul>
<li class="chapter" data-level="2.1.1" data-path="effectsize.html"><a href="effectsize.html#what-is-an-effect-size"><i class="fa fa-check"></i><b>2.1.1</b> What is an effect size?</a></li>
<li class="chapter" data-level="2.1.2" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_when_why"><i class="fa fa-check"></i><b>2.1.2</b> Why and when should effect sizes be reported?</a></li>
<li class="chapter" data-level="2.1.3" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_how_reporting"><i class="fa fa-check"></i><b>2.1.3</b> How should effect sizes be reported?</a></li>
<li class="chapter" data-level="2.1.4" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_standardized"><i class="fa fa-check"></i><b>2.1.4</b> What is a standardized effect size?</a></li>
<li class="chapter" data-level="2.1.5" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_simple_v_standardized"><i class="fa fa-check"></i><b>2.1.5</b> Should simple or standardized effect sizes be reported?</a></li>
<li class="chapter" data-level="2.1.6" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_large_enough"><i class="fa fa-check"></i><b>2.1.6</b> How do I know my effect is large enough?</a></li>
<li class="chapter" data-level="2.1.7" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_small_medium_large"><i class="fa fa-check"></i><b>2.1.7</b> What about Cohen’s small, medium, and large effect sizes?</a></li>
<li class="chapter" data-level="2.1.8" data-path="effectsize.html"><a href="effectsize.html#how-to-use-effect-sizes-in-planning-a-study"><i class="fa fa-check"></i><b>2.1.8</b> How to use effect sizes in planning a study?</a></li>
<li class="chapter" data-level="2.1.9" data-path="effectsize.html"><a href="effectsize.html#effectsize_faq_controversial"><i class="fa fa-check"></i><b>2.1.9</b> What are controversial issues about effect sizes?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_simple"><i class="fa fa-check"></i><b>2.2</b> Exemplar: Simple effect size</a><ul>
<li class="chapter" data-level="2.2.1" data-path="effectsize.html"><a href="effectsize.html#data"><i class="fa fa-check"></i><b>2.2.1</b> Data</a></li>
<li class="chapter" data-level="2.2.2" data-path="effectsize.html"><a href="effectsize.html#calculating-simple-effect-size"><i class="fa fa-check"></i><b>2.2.2</b> Calculating simple effect size</a></li>
<li class="chapter" data-level="2.2.3" data-path="effectsize.html"><a href="effectsize.html#reporting-simple-effect-size"><i class="fa fa-check"></i><b>2.2.3</b> Reporting simple effect size</a></li>
<li class="chapter" data-level="2.2.4" data-path="effectsize.html"><a href="effectsize.html#interpreting-effect-size-same-result-different-domains-different-interpretations"><i class="fa fa-check"></i><b>2.2.4</b> Interpreting effect size: same result, different domains = different interpretations</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_within"><i class="fa fa-check"></i><b>2.3</b> Exemplar: Within-subjects experiment</a><ul>
<li class="chapter" data-level="2.3.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis"><i class="fa fa-check"></i><b>2.3.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.3.2" data-path="effectsize.html"><a href="effectsize.html#simulate-a-dataset"><i class="fa fa-check"></i><b>2.3.2</b> Simulate a dataset</a></li>
<li class="chapter" data-level="2.3.3" data-path="effectsize.html"><a href="effectsize.html#a-look-at-the-data"><i class="fa fa-check"></i><b>2.3.3</b> A look at the data</a></li>
<li class="chapter" data-level="2.3.4" data-path="effectsize.html"><a href="effectsize.html#compute-effect-sizes"><i class="fa fa-check"></i><b>2.3.4</b> Compute effect sizes</a></li>
<li class="chapter" data-level="2.3.5" data-path="effectsize.html"><a href="effectsize.html#bootstrapping"><i class="fa fa-check"></i><b>2.3.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="2.3.6" data-path="effectsize.html"><a href="effectsize.html#getting-a-confidence-interval-from-a-bootstrap"><i class="fa fa-check"></i><b>2.3.6</b> Getting a confidence interval from a bootstrap</a></li>
<li class="chapter" data-level="2.3.7" data-path="effectsize.html"><a href="effectsize.html#plot-the-effect-sizes"><i class="fa fa-check"></i><b>2.3.7</b> Plot the effect sizes</a></li>
<li class="chapter" data-level="2.3.8" data-path="effectsize.html"><a href="effectsize.html#reporting-the-results"><i class="fa fa-check"></i><b>2.3.8</b> Reporting the results</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_standardized"><i class="fa fa-check"></i><b>2.4</b> Exemplar: Standardized effect size</a><ul>
<li class="chapter" data-level="2.4.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis-1"><i class="fa fa-check"></i><b>2.4.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.4.2" data-path="effectsize.html"><a href="effectsize.html#standardized-effect-size"><i class="fa fa-check"></i><b>2.4.2</b> Standardized effect size</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="effectsize.html"><a href="effectsize.html#effectsize_exemplar_nonparametric"><i class="fa fa-check"></i><b>2.5</b> Exemplar: Nonparametric effect size</a><ul>
<li class="chapter" data-level="2.5.1" data-path="effectsize.html"><a href="effectsize.html#libraries-needed-for-this-analysis-2"><i class="fa fa-check"></i><b>2.5.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="2.5.2" data-path="effectsize.html"><a href="effectsize.html#nonparametric-effect-size"><i class="fa fa-check"></i><b>2.5.2</b> Nonparametric effect size</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pvalues.html"><a href="pvalues.html"><i class="fa fa-check"></i><b>3</b> p values</a></li>
<li class="chapter" data-level="4" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4</b> Inferential statistics</a></li>
<li class="chapter" data-level="5" data-path="interpretations.html"><a href="interpretations.html"><i class="fa fa-check"></i><b>5</b> Interpretations and Conclusion</a></li>
<li class="chapter" data-level="6" data-path="preregistration.html"><a href="preregistration.html"><i class="fa fa-check"></i><b>6</b> Preregistration and prespecification</a></li>
<li class="chapter" data-level="7" data-path="multiplecomparisons.html"><a href="multiplecomparisons.html"><i class="fa fa-check"></i><b>7</b> Multiple comparisons</a></li>
<li class="chapter" data-level="8" data-path="interrater.html"><a href="interrater.html"><i class="fa fa-check"></i><b>8</b> Inter-rater reliability</a></li>
<li class="chapter" data-level="9" data-path="modelselection.html"><a href="modelselection.html"><i class="fa fa-check"></i><b>9</b> Model selection and evaluation</a></li>
<li class="chapter" data-level="10" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>10</b> Bayesian inference</a></li>
<li class="chapter" data-level="11" data-path="likert.html"><a href="likert.html"><i class="fa fa-check"></i><b>11</b> Likert-scale data</a></li>
<li class="chapter" data-level="12" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>12</b> Assumptions and robustness</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-effectsize.html"><a href="appendix-effectsize.html"><i class="fa fa-check"></i><b>A</b> Effect size appendix</a><ul>
<li class="chapter" data-level="A.1" data-path="appendix-effectsize.html"><a href="appendix-effectsize.html#appendix_effectsize_simple"><i class="fa fa-check"></i><b>A.1</b> Alternative approaches for simple effect size exemplar</a><ul>
<li class="chapter" data-level="A.1.1" data-path="appendix-effectsize.html"><a href="appendix-effectsize.html#libraries-needed-for-this-analysis-3"><i class="fa fa-check"></i><b>A.1.1</b> Libraries needed for this analysis</a></li>
<li class="chapter" data-level="A.1.2" data-path="appendix-effectsize.html"><a href="appendix-effectsize.html#data-1"><i class="fa fa-check"></i><b>A.1.2</b> Data</a></li>
<li class="chapter" data-level="A.1.3" data-path="appendix-effectsize.html"><a href="appendix-effectsize.html#calculating-simple-effect-size-1"><i class="fa fa-check"></i><b>A.1.3</b> Calculating simple effect size</a></li>
<li class="chapter" data-level="A.1.4" data-path="appendix-effectsize.html"><a href="appendix-effectsize.html#comparing-approaches"><i class="fa fa-check"></i><b>A.1.4</b> Comparing approaches</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Transparent Statistics Guidelines</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principles" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Transparent Statistics Guiding Principles</h1>
<p><a href="https://github.com/transparentstats/guidelines"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_green_007200.png" alt="Fork me on GitHub"></a></p>
<div class="preamble">
<p><strong>Version:</strong> 1.0</p>
<p><a href="https://doi.org/10.5281/zenodo.2226616"><img src="https://img.shields.io/badge/this%20version-10.5281%2Fzenodo.2226616-green.svg" alt="DOI this version" /></a> <a href="https://doi.org/10.5281/zenodo.1186169"><img src="https://img.shields.io/badge/all%20versions-10.5281%2Fzenodo.1186169-blue.svg" alt="DOI all versions" /></a> <a href="index.html#citing"><img src="https://img.shields.io/badge/-cite%20or%20contribute-lightgrey.svg" alt="Citing or contribute" /></a></p>
<p><strong>Contributed to the writing:</strong> Pierre Dragicevic, Chat Wacharamanotham, Matthew Kay</p>
<p><strong>Gave feedback:</strong></p>
<p><strong>Endorsed:</strong></p>
</div>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>Human-computer interaction (HCI) is a large, multidisciplinary field drawing on a variety of approaches for analyzing quantitative data. However, many of our existing practices have drawn increasing criticism, such as our overreliance on mechanical statistical testing procedures, our lack of replications and meta-analyses, and our unwillingness to share data and study materials. These issues have been discussed within HCI <span class="citation">(Wilson et al. <a href="#ref-Wilson2011">2011</a>; Kaptein and Robertson <a href="#ref-Kaptein2012">2012</a>; Dragicevic <a href="#ref-Dragicevic2016">2016</a>; Kay, Nelson, and Hekler <a href="#ref-Kay2016">2016</a>; Cockburn, Gutwin, and Dix <a href="#ref-Cockburn2018">2018</a>)</span> and (to a much larger extent) in other fields <span class="citation">(Cohen <a href="#ref-Cohen1994">1994</a>; Gigerenzer <a href="#ref-Gigerenzer2004">2004</a>; Ioannidis <a href="#ref-Ioannidis2005">2005</a>; Simmons, Nelson, and Simonsohn <a href="#ref-Simmons2011">2011</a>; Giner-Sorolla <a href="#ref-Giner2012">2012</a>; Cumming <a href="#ref-Cumming2014a">2014</a>; Nosek et al. <a href="#ref-Nosek2017">2017</a>)</span>. Poor statistical practice and the lack of transparency in statistical reporting hamper the progress of knowledge and undermine the scientific credibility of affected disciplines, as witnessed by the growing number of press articles reporting a “crisis of confidence” in the most visible of these disciplines <span class="citation">(Earp and Trafimow <a href="#ref-Earp2015">2015</a>)</span>.</p>
<p>The purpose of the transparent statistics guidelines is not to admonish an entire field of researchers for their existing practices nor to urge them to adopt a specific set of methods. There is no universal inference procedure that can act as a substitute for good statistical reasoning <span class="citation">(Gigerenzer and Marewski <a href="#ref-Gigerenzer2015">2015</a>)</span>. The multifaceted nature of HCI also means we need to embrace a variety of practices. A fixed set of DOs and DON’Ts would be both too brittle to change over time and too restrictive in the face of the various ways of generating knowledge in our field. Instead, we propose to advance a general vision of <em>transparent statistics</em> that HCI researchers can draw inspiration from, and that is largely method-agnostic. We refer to transparent statistics simply as <em>“a philosophy of statistical reporting whose purpose is to advance scientific knowledge rather than to persuade”</em>. Regardless of the methods used, we aim to provide guidance that makes the communication of those methods more transparent, that makes reproduction and replication of work easier, and that makes evaluation of work (e.g., by peer reviewers) easier and more fair.</p>
<p>To that end, a “transparent statistics” initiative was started in 2016, whose purpose is to discuss ways of promoting transparent statistics at CHI and suggest a series of incremental changes within the community <span class="citation">(“Transparent Statistics Website” <a href="#ref-Transparent2017">2017</a>)</span>. These include more specific author and reviewer guidelines, exemplars for authors, and “badges” <span class="citation">(“Open Science Badges” <a href="#ref-Open2017">2017</a>)</span>. The goal of the initiative is to address questions such as: what can an author do to improve the transparency of their communication? What can a reviewer do to encourage and reward transparency? What changes to the review process might encourage transparency and incentivize researchers? In this way we hope to avoid the time-honored tradition of admonishing researchers for doing statistics poorly, and instead encourage them—and guide them—to do better. The goal of this first chapter is to lay out the high-level principles on which other chapters will be based. Like other chapters, this chapter is not meant to be fixed in stone, but is meant to be constantly evolving and iteratively refined by the CHI community.</p>
</div>
<div id="guiding-principles" class="section level2">
<h2><span class="header-section-number">1.2</span> Guiding Principles</h2>
<p>Again, transparent statistics is <em>“a philosophy of statistical reporting whose purpose is to advance scientific knowledge rather than to persuade”</em>. This idea is not new. For example, the following quote from Ronald Fisher captures the essence of transparent statistics:</p>
<blockquote>
<p>“we have the duty of […] communicating our conclusions in intelligible form, in recognition of the right of other free minds to utilize them in making their own decisions.” <span class="citation">(Fisher <a href="#ref-Fisher1955">1955</a>)</span>.</p>
</blockquote>
<p>More recent writings have emphasized the importance of contributing useful and accurate knowledge over telling compelling and convincing stories <span class="citation">(Giner-Sorolla <a href="#ref-Giner2012">2012</a>; Dragicevic <a href="#ref-Dragicevic2016">2016</a>)</span>. Based on these visions, we propose a set of nine guiding principles for writing transparent statistical reports: 1) faithfulness, 2) robustness, 3) resilience, 4) process transparency, 5) clarity, 6) simplicity, 7) non-contingency, 8) precision and economy, and 9) material availability.</p>
<div id="faithfulness" class="section level3 unnumbered">
<h3>1. Faithfulness</h3>
<p>At the most basic level, a transparent statistical report should strive to be faithful to the data and the phenomena studied. This means that it should strive to capture and convey the “truth” as accurately as possible, especially concerning the uncertainty within the data. Major sources of uncertainty need to be carefully assessed and propagated to the presentations and interpretations of the results, all the way up to the final conclusions. Conclusions should be nuanced and stress the uncertainty in the data and in the process.</p>
<ul>
<li><p><em>Example:</em> It is evident that any major error in an analysis will result in findings that are likely not faithful to the data and the phenomena studied. This includes effect estimates that are very different from the true effect, but also measures of uncertainty that fail to capture the true uncertainty.</p></li>
<li><p><em>Example:</em> Exaggerating findings by presenting uncertain results as certain is unfaithful to the data.</p></li>
<li><p><em>Example:</em> A study report that analyzes all its data carefully but fails to acknowledge important issues with data validity (e.g., non-random condition assignment) is faithful to the data but unfaithful to the phenomena studied. The same goes with over-generalizing findings.</p></li>
</ul>
</div>
<div id="robustness" class="section level3 unnumbered">
<h3>2. Robustness</h3>
<p>In order to minimize the likelihood of inaccurate (unfaithful) results, data analysis and reporting strategies that are robust to departures from statistical assumptions – or that make few assumptions – should ideally be preferred.</p>
<p>Given that statistical assumptions are never met perfectly, the question should not be “are the assumptions met?” but instead “what are the likely consequences of such and such departure?”. Thus, it is hugely beneficial for researchers to know how their methods behave depending on the nature and degree of possible departures, so that they can explain it in their report when necessary. When uncertain, methods that are known for their robustness are safer choices.</p>
<ul>
<li><em>Example:</em> ANOVA is robust to the normality assumption, and can in some cases give accurate results with unusual distributions and very small sample sizes <span class="citation">(Norman <a href="#ref-Norman2010">2010</a>)</span>.</li>
<li><em>Example:</em> Bootstrapping makes no assumption about data distribution and is robust to departures from its own statistical assumptions, even though these assumptions are implausible <span class="citation">(Kirby and Gerlanc <a href="#ref-Kirby2013">2013</a>)</span>.</li>
</ul>
</div>
<div id="resilience" class="section level3 unnumbered">
<h3>3. Resilience</h3>
<p>Data analysis and reporting strategies should be resilient to statistical noise, i.e., they should yield similar outcomes across hypothetical replications of the same study. Researchers should ask themselves how their statistical report would change if they took another random sample from the same population, and should try to make claims that are as robust as possible to these changes.</p>
<p>In practice, the principle of resilience implies that researchers should avoid presenting statistical noise as signal, either by overfitting, or by overinterpreting patterns in results. It also implies that study reports should be smooth functions of the data. This means that data analysis and presentation strategies should be chosen so that similar experimental datasets yield similar results, interpretations and conclusions <span class="citation">(Dixon <a href="#ref-Dixon2003">2003</a>; Dragicevic <a href="#ref-Dragicevic2016">2016</a>; “Statistical Dances: Why No Statistical Analysis Is Reliable and What to Do About It” <a href="#ref-Dragicevic2017">2017</a>)</span>. The principle of resilience is important and is directly relevant to the issue of study replicability.</p>
<ul>
<li><em>Example:</em> Presenting a bar chart of means without error bars and commenting on the emerging patterns is akin to overfitting and is thus not resilient.</li>
<li><em>Example:</em> Computing and reporting 95% interval estimates is resilient, but drawing binary conclusions based on whether they contain zero is not, because two very similar datasets may yield seemingly very different scientific conclusions <span class="citation">(Cumming <a href="#ref-Cumming2013a">2013</a>)</span>.</li>
<li><em>Example:</em> For the same reasons, computing Bayes factors and interpreting them strictly based on conventional thresholds violates the principle of resilience <span class="citation">(“Dance of the Bayes Factors” <a href="#ref-Lakens2016">2016</a>)</span>.</li>
</ul>
</div>
<div id="process-transparency" class="section level3 unnumbered">
<h3>4. Process Transparency</h3>
<p>A core aspect of transparent statistics is that data analysis and reporting strategies need to be explained rather than implied. The decisions made during the analysis and report writing should be communicated as explicitly as possible, as the results of an analysis cannot be fairly assessed and understood if many decisions remain concealed <span class="citation">(Giner-Sorolla <a href="#ref-Giner2012">2012</a>; Gelman and Loken <a href="#ref-Gelman2013">2013</a>)</span>.</p>
<p>At the most basic level, researchers should ideally state which portions of their data analysis were planned before the data was seen, and which portions were not. Analyses that are fully planned can be referred to as <em>prespecified</em>, while analyses that are largely unplanned should be referred to as <em>exploratory</em> <span class="citation">(Cumming <a href="#ref-Cumming2014a">2014</a>, 10)</span>. Both types of analysis are valid, although the former allows to support stronger claims than the latter.</p>
<p>Process transparency also implies faithfully reporting what were the research goals, the research questions, and (optionally) the researcher’s expectations prior to seeing the data <span class="citation">(Kerr <a href="#ref-Kerr1998">1998</a>; Gelman and Loken <a href="#ref-Gelman2013">2013</a>; Cockburn, Gutwin, and Dix <a href="#ref-Cockburn2018">2018</a>)</span>. Results from analyses need to be reported whether or not they meet the researcher’s initial expectations. When this is not the case, the rationale for selecting results needs be explained. Finally, sharing data and analysis scripts greatly benefits process transparency.</p>
<ul>
<li><em>Example:</em> Hypothesizing after the results are known (or “HARKing”) strongly goes against process transparency <span class="citation">(Kerr <a href="#ref-Kerr1998">1998</a>)</span>. Researchers who do not have clear expectations should state research questions instead of hypotheses <span class="citation">(Cumming <a href="#ref-Cumming2013a">2013</a>)</span>.</li>
<li><em>Example:</em> Cherry-picking “convenient” results (e.g., results that best support the hypotheses), or trying multiple alternative analyses and reporting only those that are convenient clearly violates process transparency <span class="citation">(Simmons, Nelson, and Simonsohn <a href="#ref-Simmons2011">2011</a>)</span>.</li>
<li><em>Example:</em> Even when a researcher has no preference for a given hypothesis and no intention to p-hack, cherry-picking results to give the impression of a coherent story also goes against process transparency <span class="citation">(Giner-Sorolla <a href="#ref-Giner2012">2012</a>; Gelman and Loken <a href="#ref-Gelman2013">2013</a>)</span>.</li>
<li><em>Example:</em> Provided an analysis is presented as exploratory, trying multiple analyses and reporting the most interesting and informative results by taking a neutral stance is perfectly acceptable and does not go against process transparency <span class="citation">(Tukey <a href="#ref-Tukey1977">1977</a>)</span>. Transparency can however be increased by explaining what has been tried.</li>
</ul>
</div>
<div id="clarity" class="section level3 unnumbered">
<h3>5. Clarity</h3>
<p>Study reports should be as easy to understand as possible, because as explained by Ronald Fisher (quoted above), readers and reviewers cannot judge an analysis without understanding. There are two facets of clarity: <em>ease of processing</em> and <em>accessibility</em>.</p>
<p>Study reports should be easy to process, even when they target experts. When results can be communicated more effectively with visual representations than with numerals, visual representations should be preferred <span class="citation">(Loftus <a href="#ref-Loftus1993">1993</a>; Gelman, Pasarica, and Dodhia <a href="#ref-Gelman2002">2002</a>)</span>. Although a study report should communicate as much relevant information as possible, information overload must be avoided by reporting non-essential information in appendices or in supplemental material.</p>
<p>Second, study reports should ideally be accessible to most members of the HCI community, instead of being comprehensible by only a handful of specialists. The more accessible an analysis is, the more the “free minds” who can judge it. Thus a study report should be more an exercise of pedagogy than an exercise of rhetoric. The goal of a statistical report is not to signal expertise, but to explain.</p>
<ul>
<li><em>Example:</em> Using statistics for defensive purposes by generating p-cluttered reports rather than informative plots violates the principle of clarity.</li>
<li><em>Example:</em> Excessive numbers of significant digits are difficult to process thus they go against the principle of clarity <span class="citation">(Ehrenberg <a href="#ref-Ehrenberg1977">1977</a>)</span>, in addition to giving a misleading impression of precision <span class="citation">(Taylor <a href="#ref-Taylor1997">1997</a>)</span>.</li>
</ul>
</div>
<div id="principles_simplicity" class="section level3 unnumbered">
<h3>6. Simplicity</h3>
<p>When choosing between two data analysis procedures, the simplest procedure should ideally be preferred even if it is slightly inferior in other respects. A focus on simplicity follows from the principles of clarity and ease of processing, and it makes both researcher mistakes and reader misinterpretations less likely to occur. In other words, the KISS principle (Keep It Simple, Stupid) is as relevant in transparent statistics as in any other domain.</p>
</div>
<div id="non-contingency" class="section level3 unnumbered">
<h3>7. Non-contingency</h3>
<p>When possible and outside exploratory analyses, data analysis and reporting strategies should avoid decisions that are contingent on data, e.g., “if the data turns out like this, compute this, or report that”. This principle follows from the principles of process transparency, clarity, and simplicity, because data-contingent procedures are hard to explain and easy to leave unexplained <span class="citation">(Gelman and Loken <a href="#ref-Gelman2013">2013</a>)</span>. It is also a corollary of the principle of resilience because any dichotomous decision decreases a procedure’s resilience to statistical noise.</p>
<p>Carefully planning an analysis is a good way to make sure that the principle of non-contingency is met <span class="citation">(Cumming <a href="#ref-Cumming2014a">2014</a>)</span>, especially if all the analysis code has been written ahead of time based on pilot data <span class="citation">(Dragicevic <a href="#ref-Dragicevic2016">2016</a>)</span>. Pre-registering an analysis further increases transparency by allowing anyone to verify that the plan has been followed <span class="citation">(Nosek et al. <a href="#ref-Nosek2017">2017</a>; Cockburn, Gutwin, and Dix <a href="#ref-Cockburn2018">2018</a>)</span>. In exploratory analyses and in complex modeling problems, which are often data-contingent by nature, the principle of non-contingency should be applied to the best effort.</p>
<ul>
<li><em>Example:</em> Using a test of normality to decide whether to use parametric or non-parametric methods violates the principle of non-contingency, in addition to not being very useful <span class="citation">(Stewart-Oaten <a href="#ref-Stewart1995">1995</a>; Wierdsma <a href="#ref-Wierdsma2013">2013</a>)</span>. If the test of normality is not mentioned in the report, it additionally violates the principle of process transparency.</li>
<li><em>Example:</em> Selective reporting of data (i.e., cherry-picking) clearly violates the non-contingency principle, and generally also the principles of faithfulness and of process transparency. It is only acceptable if the analysis is clearly presented as exploratory, and if the goal of the selection is to learn from the data rather than to support a convenient hypothesis or story.</li>
</ul>
</div>
<div id="precision-and-economy" class="section level3 unnumbered">
<h3>8. Precision and economy</h3>
<p>Data quality <span class="citation">(Gelman <a href="#ref-Gelman2017">2017</a>)</span> and high statistical power <span class="citation">(Cohen <a href="#ref-Cohen1994">1994</a>)</span>, which in the estimation world translates to high statistical precision <span class="citation">(Cumming <a href="#ref-Cumming2013a">2013</a>; Kruschke and Liddell <a href="#ref-Kruschke2017">2017</a>)</span>, are important goals to pursue. This is because even if full transparency is achieved, a study report where nothing conclusive can be said would be a waste of readers’ time, and may prompt them to seek inexistent patterns. Precision depends on experiment design, but also on the choice of analysis methods – thus analysis methods that yield high precision should be prefered. However, researchers should strive to avoid false precision, e.g., reporting numerical results without information about their uncertainty and/or with way more significant digits than justified by their uncertainty <span class="citation">(Taylor <a href="#ref-Taylor1997">1997</a>)</span>.</p>
<p>Analysis and reporting strategies that waste statistical power and precision (e.g., by dichotomizing continuous variables) should also be ideally avoided <span class="citation">(Dragicevic <a href="#ref-Dragicevic2016">2016</a>)</span>. Though the economy principle is not directly related to transparency, it is generally advisable not to waste data. It is a sensible goal for researchers to try to learn as much as possible from a study, provided that the principles of faithfulness and process transparency are carefully kept in mind. For similar reasons, while it is essential that researchers do not read too much in their data and do not fall for confirmation bias, exploratory analyses are often very informative and should thus be encouraged. The best study reports combine prespecified with exploratory analyses, while clearly distinguishing between the two.</p>
</div>
<div id="material-availability" class="section level3 unnumbered">
<h3>9. Material availability</h3>
<p>Sharing as much study material as possible is a core part of transparent statistics, as it greatly facilitates peer scrutiny and replication. Being able to run the experimental software and examine what participants saw (the techniques, tasks, instructions, and questions asked) is essential in order for other researchers to understand the details of a study. In addition, sharing the source code of the experimental software greatly facilitates replication. Similarly, experimental data (all data files and if possible analysis scripts) is necessary for conducting re-analyses and meta-analyses. Although uploading supplementary material makes sense during the reviewing phase, to be really useful all material should be freely shared online upon paper acceptance, ideally on a website that can guarantee long-term accessibility.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Cockburn2018">
<p>Cockburn, Andy, Karl Gutwin, and Alan Dix. 2018. “HARK No More: On the Preregistration of Chi Experiments.” ACM.</p>
</div>
<div id="ref-Cohen1994">
<p>Cohen, Jacob. 1994. “The Earth Is Round (P&lt;.05).” <em>American Psychologist</em> 49 (12). American Psychological Association: 997. <a href="http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf" class="uri">http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf</a>.</p>
</div>
<div id="ref-Cumming2013a">
<p>Cumming, Geoff. 2013. <em>Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis</em>. Routledge.</p>
</div>
<div id="ref-Cumming2014a">
<p>Cumming, Geoff. 2014. “The New Statistics: Why and How.” <em>Psychological Science</em> 25 (1): 7–29. doi:<a href="https://doi.org/10.1177/0956797613504966">10.1177/0956797613504966</a>.</p>
</div>
<div id="ref-Lakens2016">
<p>“Dance of the Bayes Factors.” 2016. <a href="http://daniellakens.blogspot.fr/2016/07/dance-of-bayes-factors.html" class="uri">http://daniellakens.blogspot.fr/2016/07/dance-of-bayes-factors.html</a>. <a href="http://daniellakens.blogspot.fr/2016/07/dance-of-bayes-factors.html" class="uri">http://daniellakens.blogspot.fr/2016/07/dance-of-bayes-factors.html</a>.</p>
</div>
<div id="ref-Dixon2003">
<p>Dixon, Peter. 2003. “The P-Value Fallacy and How to Avoid It.” <em>Canadian Journal of Experimental Psychology/Revue Canadienne de Psychologie Experimentale</em> 57 (3). Canadian Psychological Association: 189. <a href="https://www.ncbi.nlm.nih.gov/pubmed/14596477" class="uri">https://www.ncbi.nlm.nih.gov/pubmed/14596477</a>.</p>
</div>
<div id="ref-Dragicevic2016">
<p>Dragicevic, Pierre. 2016. “Fair Statistical Communication in Hci.” In <em>Modern Statistical Methods for Hci</em>, 291–330. Springer. <a href="https://hal.inria.fr/hal-01377894/document" class="uri">https://hal.inria.fr/hal-01377894/document</a>.</p>
</div>
<div id="ref-Earp2015">
<p>Earp, Brian D, and David Trafimow. 2015. “Replication, Falsification, and the Crisis of Confidence in Social Psychology.” <em>Frontiers in Psychology</em> 6. Frontiers Media SA. <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00621/full" class="uri">https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00621/full</a>.</p>
</div>
<div id="ref-Ehrenberg1977">
<p>Ehrenberg, ASC. 1977. “Rudiments of Numeracy.” <em>Journal of the Royal Statistical Society. Series A (General)</em>. JSTOR, 277–97. <a href="http://www1.maths.leeds.ac.uk/~sta6ajb/math1910/p4.pdf" class="uri">http://www1.maths.leeds.ac.uk/~sta6ajb/math1910/p4.pdf</a>.</p>
</div>
<div id="ref-Fisher1955">
<p>Fisher, Ronald. 1955. “Statistical Methods and Scientific Induction.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>. JSTOR, 69–78. <a href="http://www.ssnpstudents.com/wp/wp-content/uploads/2015/02/Fisher-1955.pdf" class="uri">http://www.ssnpstudents.com/wp/wp-content/uploads/2015/02/Fisher-1955.pdf</a>.</p>
</div>
<div id="ref-Gelman2017">
<p>Gelman, Andrew. 2017. “Ethics and Statistics: Honesty and Transparency Are Not Enough.” <em>Chance</em> 30 (1). Taylor &amp; Francis: 37–39. <a href="http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics14.pdf" class="uri">http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics14.pdf</a>.</p>
</div>
<div id="ref-Gelman2013">
<p>Gelman, Andrew, and Eric Loken. 2013. “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘P-Hacking’ and the Research Hypothesis Was Posited Ahead of Time.” <em>Department of Statistics, Columbia University</em>. <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf" class="uri">http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf</a>.</p>
</div>
<div id="ref-Gelman2002">
<p>Gelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” <em>The American Statistician</em> 56 (2). Taylor &amp; Francis: 121–30. <a href="https://pdfs.semanticscholar.org/202c/fec06a87fc96d3d56b6ad2ba4237b3fde141.pdf" class="uri">https://pdfs.semanticscholar.org/202c/fec06a87fc96d3d56b6ad2ba4237b3fde141.pdf</a>.</p>
</div>
<div id="ref-Gigerenzer2004">
<p>Gigerenzer, Gerd. 2004. “Mindless Statistics.” <em>The Journal of Socio-Economics</em> 33 (5). Elsevier: 587–606. <a href="http://pubman.mpdl.mpg.de/pubman/item/escidoc:2101336/component/escidoc:2101335/GG_Mindless_2004.pdf" class="uri">http://pubman.mpdl.mpg.de/pubman/item/escidoc:2101336/component/escidoc:2101335/GG_Mindless_2004.pdf</a>.</p>
</div>
<div id="ref-Gigerenzer2015">
<p>Gigerenzer, Gerd, and Julian N Marewski. 2015. “Surrogate Science: The Idol of a Universal Method for Scientific Inference.” <em>Journal of Management</em> 41 (2). Sage Publications Sage CA: Los Angeles, CA: 421–40. <a href="http://www.dcscience.net/Gigerenzer-Journal-of-Management-2015.pdf" class="uri">http://www.dcscience.net/Gigerenzer-Journal-of-Management-2015.pdf</a>.</p>
</div>
<div id="ref-Giner2012">
<p>Giner-Sorolla, Roger. 2012. “Science or Art? How Aesthetic Standards Grease the Way Through the Publication Bottleneck but Undermine Science.” <em>Perspectives on Psychological Science</em> 7 (6). Sage Publications Sage CA: Los Angeles, CA: 562–71. <a href="http://journals.sagepub.com/doi/full/10.1177/1745691612457576" class="uri">http://journals.sagepub.com/doi/full/10.1177/1745691612457576</a>.</p>
</div>
<div id="ref-Ioannidis2005">
<p>Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” <em>PLoS Medicine</em> 2 (8). Public Library of Science: e124. <a href="http://robotics.cs.tamu.edu/RSS2015NegativeResults/pmed.0020124.pdf" class="uri">http://robotics.cs.tamu.edu/RSS2015NegativeResults/pmed.0020124.pdf</a>.</p>
</div>
<div id="ref-Kaptein2012">
<p>Kaptein, Maurits, and Judy Robertson. 2012. “Rethinking Statistical Analysis Methods for Chi.” In <em>Proceedings of the Sigchi Conference on Human Factors in Computing Systems</em>, 1105–14. ACM. <a href="http://judyrobertson.typepad.com/files/chi2012_submission_final.pdf" class="uri">http://judyrobertson.typepad.com/files/chi2012_submission_final.pdf</a>.</p>
</div>
<div id="ref-Kay2016">
<p>Kay, Matthew, Gregory L Nelson, and Eric B Hekler. 2016. “Researcher-Centered Design of Statistics: Why Bayesian Statistics Better Fit the Culture and Incentives of Hci.” In <em>Proceedings of the 2016 Chi Conference on Human Factors in Computing Systems</em>, 4521–32. ACM. <a href="http://www.mjskay.com/papers/chi_2016_bayes.pdf" class="uri">http://www.mjskay.com/papers/chi_2016_bayes.pdf</a>.</p>
</div>
<div id="ref-Kerr1998">
<p>Kerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” <em>Personality and Social Psychology Review</em> 2 (3). Sage Publications Sage CA: Los Angeles, CA: 196–217. <a href="http://www.socialrelationslab.com/uploads/1/8/9/6/18966149/harkingkerr1998.pdf" class="uri">http://www.socialrelationslab.com/uploads/1/8/9/6/18966149/harkingkerr1998.pdf</a>.</p>
</div>
<div id="ref-Kirby2013">
<p>Kirby, Kris N, and Daniel Gerlanc. 2013. “BootES: An R Package for Bootstrap Confidence Intervals on Effect Sizes.” <em>Behavior Research Methods</em> 45 (4). Springer: 905–27. <a href="http://web.williams.edu/Psychology/Faculty/Kirby/bootes-kirby-gerlanc-in-press.pdf" class="uri">http://web.williams.edu/Psychology/Faculty/Kirby/bootes-kirby-gerlanc-in-press.pdf</a>.</p>
</div>
<div id="ref-Kruschke2017">
<p>Kruschke, John K, and Torrin M Liddell. 2017. “The Bayesian New Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power Analysis from a Bayesian Perspective.” <em>Psychonomic Bulletin &amp; Review</em>. Springer, 1–29. <a href="https://osf.io/ksfyr/download?format=pdf" class="uri">https://osf.io/ksfyr/download?format=pdf</a>.</p>
</div>
<div id="ref-Loftus1993">
<p>Loftus, Geoffrey R. 1993. “A Picture Is Worth a Thousand P Values: On the Irrelevance of Hypothesis Testing in the Microcomputer Age.” <em>Behavior Research Methods, Instruments, &amp; Computers</em> 25 (2). Springer: 250–56. <a href="https://faculty.washington.edu/gloftus/Research/Publications/Manuscript.pdf/Loftus%20p-values%201993.pdf">https://faculty.washington.edu/gloftus/Research/Publications/Manuscript.pdf/Loftus%20p-values%201993.pdf</a>.</p>
</div>
<div id="ref-Norman2010">
<p>Norman, Geoff. 2010. “Likert Scales, Levels of Measurement and the ‘Laws’ of Statistics.” <em>Advances in Health Sciences Education</em> 15 (5). Springer: 625–32. <a href="https://pdfs.semanticscholar.org/6dc0/0756ab722370b815df1223f4044dd63841a8.pdf" class="uri">https://pdfs.semanticscholar.org/6dc0/0756ab722370b815df1223f4044dd63841a8.pdf</a>.</p>
</div>
<div id="ref-Nosek2017">
<p>Nosek, Brian A, Charles R Ebersole, Alexander DeHaven, and David Mellor. 2017. “The Preregistration Revolution.” Open Science Framework. <a href="https://osf.io/2dxu5/download?format=pdf" class="uri">https://osf.io/2dxu5/download?format=pdf</a>.</p>
</div>
<div id="ref-Open2017">
<p>“Open Science Badges.” 2017. <a href="https://cos.io/our-services/open-science-badges/" class="uri">https://cos.io/our-services/open-science-badges/</a>. <a href="https://cos.io/our-services/open-science-badges/" class="uri">https://cos.io/our-services/open-science-badges/</a>.</p>
</div>
<div id="ref-Simmons2011">
<p>Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” <em>Psychological Science</em> 22 (11). Sage Publications Sage CA: Los Angeles, CA: 1359–66. <a href="http://opim.wharton.upenn.edu/DPlab/papers/publishedPapers/Simmons_2011_False-Positive%20Psychology.pdf">http://opim.wharton.upenn.edu/DPlab/papers/publishedPapers/Simmons_2011_False-Positive%20Psychology.pdf</a>.</p>
</div>
<div id="ref-Dragicevic2017">
<p>“Statistical Dances: Why No Statistical Analysis Is Reliable and What to Do About It.” 2017. <a href="https://tinyurl.com/gricad-dance" class="uri">https://tinyurl.com/gricad-dance</a>. <a href="https://tinyurl.com/gricad-dance" class="uri">https://tinyurl.com/gricad-dance</a>.</p>
</div>
<div id="ref-Stewart1995">
<p>Stewart-Oaten, Allan. 1995. “Rules and Judgments in Statistics: Three Examples.” <em>Ecology</em> 76 (6). Wiley Online Library: 2001–9. <a href="http://onlinelibrary.wiley.com/doi/10.2307/1940736/full" class="uri">http://onlinelibrary.wiley.com/doi/10.2307/1940736/full</a>.</p>
</div>
<div id="ref-Taylor1997">
<p>Taylor, John. 1997. <em>Introduction to Error Analysis, the Study of Uncertainties in Physical Measurements</em>. University Science Books.</p>
</div>
<div id="ref-Transparent2017">
<p>“Transparent Statistics Website.” 2017. <a href="http://transparentstatistics.org/" class="uri">http://transparentstatistics.org/</a>.</p>
</div>
<div id="ref-Tukey1977">
<p>Tukey, John W. 1977. “Exploratory Data Analysis.” Reading, Mass.</p>
</div>
<div id="ref-Wierdsma2013">
<p>Wierdsma, A. 2013. “What Is Wrong with Tests of Normality?” <a href="http://tinyurl.com/normality-wrong" class="uri">http://tinyurl.com/normality-wrong</a>. <a href="http://tinyurl.com/normality-wrong" class="uri">http://tinyurl.com/normality-wrong</a>.</p>
</div>
<div id="ref-Wilson2011">
<p>Wilson, Max L, Wendy Mackay, Ed Chi, Michael Bernstein, Dan Russell, and Harold Thimbleby. 2011. “RepliCHI-Chi Should Be Replicating and Validating Results More: Discuss.” In <em>CHI’11 Extended Abstracts on Human Factors in Computing Systems</em>, 463–66. ACM. <a href="https://hal.inria.fr/file/index/docid/1000423/filename/RepliCHI-panel-2011.pdf" class="uri">https://hal.inria.fr/file/index/docid/1000423/filename/RepliCHI-panel-2011.pdf</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="effectsize.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/transparentstats/guidelines/edit/master/guides/principles.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
