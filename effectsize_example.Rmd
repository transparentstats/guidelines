---
title: "Effect size"
output: html_notebook
bibliography: references.bib
---
```{r}
library(tidyverse)
```

# What is effect size?
Broadly speaking, an effect size is *"anything that might be of interest"* [@Cumming2013a], i.e., a quantity that captures the practical magnitude of the effect studied. For example, *the difference* between task completion time (in seconds) or *the difference* between error rate (in percent).

Effect sizes are not as scary as you think. Effect sizes are *essentially* any way to compute the practical size of an effect. It can be something as simple as the difference between means between two treatments --- and often this is a very informative measure that can tell you whether a treatment has an effect large enough to care about. 

The term *effect size* is an overloaded term: sometimes it is used to refer to *standardized* effect sizes, like Cohen’s *d*, and sometimes to *simple* effect sizes, like the difference between two means. This can lead to confusion.

# Should I report simple effect sizes or standardized effect sizes?
Cohen’s *d* --- the difference in means divided by the standard deviation --- is sometimes called a standardized effect size. Standard effect sizes are useful in some situations, for example when effects expressed in different units need to be combined or compared [@Cumming2014a] However, often reporting a simple effect size is easier to interpret [@Cumming2014a]. When the units of the data are meaningful, reporting simple effect sizes can make it easier to judge whether the size of the effect has practical significance [@Wilkinson1999a].

If you are reporting standardized effect sizes, and there is no inherent reasoning to argue for a particular interpretation of the practical significance of the standardized effect size, you should find another way to assess the practical significance of the effect.


# How do you know it is large enough?
Deciding if an effect size is “large enough” often requires expert judgement. Is a difference of 100ms a large difference in reaction time? Is a difference of 100ms a large difference in time to receive a chat message? 

Often it is difficult to determine whether a certain effect is of practical importance. For example, a difference in pointing time of 100ms between two pointing techniques can be large or small depending on the application, how often it is used, the context of use, etc. In that case, it is enough to present effect sizes in a clear way and leave the judgment of practical importance to the reader.

While some informal thresholds are used for Cohen’s *d* (see "Why and when should effect sizes be reported?"), these thresholds are largely arbitrary [verify] and may not truly be domain agnostic. Those thresholds were originally proposed by Cohen based on a dataset of human heights [verify that]. Cohen himself noted that these thresholds may not be directly applicable to other domains.

More generally, it is beneficial to avoid the use of arbitrary thresholds or dichotomous thinking when deciding on whether an effect is large enough, and instead try to think whether effect is of practical importance. This requires domain knowledge and analysis. 

In practice, a researcher could (should) think in advance what would be an effect they would like to see in their data from the treatment (see "planned analyses" and "power analysis"). This would be “large enough”. (Obviously, this is subjective, and readers/reviewers may disagree.)



# Why and when should effect sizes be reported?
Taken in a broad sense, effect sizes should (by definition) be reported in quantitative research unless they are good reasons not to do so. Effect sizes are essential for understanding the practical effect or differences in measurements. Identifying the effect size(s) of interest allows turning a research question into a precise, quantitative question. For example, if a researcher is interested in showing that her technique is faster than a baseline technique, an appropriate choice of effect size is the mean difference in completion times.

For data that is sensible to characterize with means and standard deviations, an effect size can be computed between two means M1 and M2 as:

<center> $d = \frac{\left|M_{1} - M_{2}\right|}{SD_{\text{pool}}}$</center>

The above effect size, known as Cohen’s *d*, divides the absolute mean difference by $SD_{\text{pool}}$, the standard deviation of the pooled data points, (although there are [other approaches](http://stats.idre.ucla.edu/other/mult-pkg/faq/general/effect-size-power/faqhow-is-effect-size-used-in-power-analysis/) to the standardization). Traditionally, a *d* of 0.2--0.5 is “small,” 0.5--0.8 is “medium,” and 0.80--1.0+ is “large” [@Cohen1988a]. However, one should not blindly interpret effect size values. Instead, one should carefully consider what the practical difference is, and whether it matters in the problem domain.

Sometimes, the main focus of a study is on the direction (rather than magnitude) of the effect and the effect sizes are hard to compute or to interpret. In such cases, practical significance should be the focus.



# How should effect sizes be reported?
There are many ways to report effect sizes. One can choose to report simple effect sizes or standardized effect sizes (see “Why and when should effect sizes be reported?”). Effect sizes can be reported numerically or graphically. They are all acceptable, although plots tend to be easier to comprehend than numbers, and simple effect sizes are easier to interpret than standardized effect sizes for measures reported in familiar units such as completion times [@Wilkinson1999a, @Cumming2013a, Matt's ref]. 

Effect size should be reported together with an appropriate measure of error to show the degree of uncertainty. For example, when reporting the mean difference, also report 95% confidence interval of difference.


# Issues with effect sizes?
```
Draft:

Discuss here about Cohen’s delta, and how Cohen himself did not want people to use small, medium and large labels
```


# Drawbacks to effect sizes 
```
Draft:
How about region of practical equivalence (ROPE)
http://doingbayesiandataanalysis.blogspot.com/2013/08/how-much-of-bayesian-posterior.html

```


# How Do I Calculate an Effect Size for A NonParametric Test?
For a nonparametric test that produces a Z-score, like the Mann-Whitney U test or the Wilcoxon Signed-Rank test, an effect size can be computed as:

<center>$r = \left|\frac{Z}{\sqrt{N}}\right|$</center>

Above, Z is the Z-score and N is the number of observations in all groups [@Rosenthal1991a, p. 19). The result, *r*, is a variance-based effect size, like Pearson *r*, not a Cohen *d*-family effect size. The *r* can be squared to estimate the percentage of variance explained, however it will not be exactly equivalent to the Pearson *r*. 

According to [@Cohen1988a], an *r* of 0.1 is “small,” of 0.3 is “medium,” and of 0.5 is “large.” However, one should take care when interpreting effect sizes rotely. Instead, consider the practical differences emerging in one’s study and whether those differences are meaningful in the world.


## Data
Assuming between-subjects design:
```{r}
set.seed(12)
n <- 20
data <- tibble(
  Group = rep(c("A", "B"), each = n),
  TaskCompletionTime_Sec = c(
    rnorm(n, mean = 12, sd = 1),
    rnorm(n, mean = 10, sd = 2)
  )
)
```

## Visualizing data

```{r, fig.height=2, fig.width=3}
pd <- position_dodge(0.2)
data %>% 
  ggplot(aes(x = Group, y = TaskCompletionTime_Sec)) +
  geom_point(alpha = 0.2, position = pd) +
  stat_summary(fun.data = "mean_cl_normal", geom = "pointrange") +
  coord_flip() +
  ylab("Task Completion Time (s)")


```

## Simple effect size

```{r}
library(broom) # for tidy()
data_A <- (data %>% filter(Group == "A"))[[2]]
data_B <- (data %>% filter(Group == "B"))[[2]]
t_result <- tidy(t.test(data_A, data_B))
t_result # result in tabular format
```

*Simple effect size:* The task completion time of the two groups differ by `r t_result$estimate` (95% confidence interval [`r t_result$conf.low`, `r t_result$conf.high`]) seconds.

```{r, fig.height=1, fig.width=3}
t_result %>% 
  ggplot(aes(x = 1, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, color = "red") +
  coord_flip() +
  ylab("Difference in task completion time (s)") +
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

## Standardized effect size

```{r}
library(effsize)
cohen_d <- cohen.d(TaskCompletionTime_Sec ~ Group, data = data)

# manual calculation
data_A <- (data %>% filter(Group == "A"))[["TaskCompletionTime_Sec"]]
data_B <- (data %>% filter(Group == "B"))[["TaskCompletionTime_Sec"]]
sd_A <- sd(data_A)
sd_B <- sd(data_B)
sd_pool <- sqrt( (sd_A^2 + sd_B^2) / 2 )
cohen_d_manual <- abs(mean(data_A) - mean(data_B))/sd_pool
```

**Standardized effect size:** Cohen's d = `r cohen_d$estimate` SDs with 95% confidence interval [`r cohen_d$conf.int[1]` , `r cohen_d$conf.int[2]`]



## Non-parametric effect size

```{r}
library(coin)
data_A <- (data %>% filter(Group == "A"))[["TaskCompletionTime_Sec"]]
data_B <- (data %>% filter(Group == "B"))[["TaskCompletionTime_Sec"]]
wilcox_result <- wilcox_test(TaskCompletionTime_Sec ~ factor(Group), data = data)
effect_r <- abs(wilcox_result@statistic@teststatistic / sqrt(nrow(data)))
```

**Non-parametric effect size:** Variance-based effect size *r*  = `r effect_r`.


# References

