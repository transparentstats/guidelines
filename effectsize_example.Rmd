---
title: "Effect size"
output: html_notebook
bibliography: references.bib
---
```{r}
library(tidyverse)
```

# What is an effect size?
Broadly speaking, an effect size is *"anything that might be of interest"* [@Cumming2013a], that is, any quantity that captures the magnitude of an effect that is being studied. Examples include the mean difference (in seconds) in task completion times between two techniques, or the mean difference in error rates (in percent). These are called *simple effect sizes* or *unstandardized effect sizes*. More complex measures exist that are called *standardized effect sizes* (see [What is a standardized effect size?](#standarized)).

The term "effect size" is sometimes used to refer to standardized effect sizes only, which can be confusing. It is preferable to use the term "effect size" in a broader sense to include simple effect sizes [@Cumming2013a, @Wilkinson1999a].

# <a name="whenwhy"></a>Why and when should effect sizes be reported?
In quantitative experiments, effect sizes are among the most elementary and the most essential summary statistics that can be reported. Identifying the effect size(s) of interest also allows the researcher to turn a vague research question into a precise, quantitative question [@Cumming2014a]. For example, if a researcher is interested in showing that her technique is faster than a baseline technique, an appropriate choice of effect size is the mean difference in completion times. The observed effect size will indicate not only the likely direction of the effect (e.g., whether the technique is faster), but also whether the effect is large enough to care about. 

For the sake of transparency, effect sizes (simple or standardized) should always be reported in quantitative research, unless there are good reasons not to do so. According to the American Psychological Association:

> For the reader to appreciate the magnitude or importance of a study's findings, it is almost always necessary to include some measure of effect size in the Results section. [@APA2001]

Sometimes, effect sizes can be hard to compute or to interpret. When this is the case, and if the main focus of the study is on the direction (rather than magnitude) of the effect, reporting the results of statistical significance tests without reporting effect sizes (see the [inferential statistics FAQ]()) may be an acceptable option [citations needed].

# <a name="standardized"></a>What is a standardized effect size?

A standardized effect size is a unitless measure of effect size. The most common measure of standardized effect size is Cohen’s *d*, where the mean difference is divided by the standard deviation of the pooled observations [@Cohen1988a]. [Other approaches](http://stats.idre.ucla.edu/other/mult-pkg/faq/general/effect-size-power/faqhow-is-effect-size-used-in-power-analysis/) to standardization exist [prefer citations]. To some extent, standardized effect sizes make it possible to compare different studies in terms of how “impressive” their results are (see [How do I know my effect is large enough?](#howlarge)).

# Should I report simple or standardized effect sizes?

Standardized effect sizes are useful in some situations, for example when effects obtained from different experiments and/or expressed in different units need to be combined or compared [@Cumming2014a]. However, simple effect sizes are often easier to interpret [@Cumming2014a]. When the units of the data are meaningful (e.g., seconds), reporting effect sizes expressed in their original units is more informative and can make it easier to judge whether the effect has a practical significance [@Wilkinson1999a].

In most cases, simple effect sizes should be preferred over standardized effect sizes:

> Only rarely will uncorrected standardized effect size be more useful than simple effect size. It is usually far better to report simple effect size. [@baguley2009standardized]

(perhaps a bit more on drawbacks of standardized effect sizes -- Matt?)


# How should effect sizes be reported?

Simple and standardized effect sizes can be reported either numerically or graphically. Both formats are acceptable, although plots tend to be easier to comprehend than numbers when more than one effect size needs to be conveyed [citations needed]. Unless precise numerical values are important, it is sufficient to report all effect sizes graphically [citations needed].

Whether it is reported numerically or graphically, an effect size expressed as a single quantity (e.g., a difference in means) is typically only a point estimate, i.e., it is merely a “best guess” of the true effect size. Therefore, it is crucial to also assess and report the uncertainty about this effect. See the [inferential statistics FAQ]() on how to do so.


# <a name="whenlarge"></a>How do I know my effect is large enough?

Some rules of thumb exist to help interpret Cohen's *d*'s. For example, a Cohen's *d* of 0.2 to 0.5 is traditionally considered to be small, while a Cohen's *d* of 0.8 or more is generally considered to be large [@Cohen1988a]. However, these are nothing more than rough rules of thumb. Cohen initially proposed these thresholds based on a dataset of human heights, and he himself noted that they may not be applicable to other domains [citation needed].

It is generally advisable to avoid the use of arbitrary thresholds when deciding on whether an effect is large enough [citations needed], and instead try to think of whether the effect is of practical importance. This requires domain knowledge, and often a fair degree of subjective judgment. Ideally, a researcher should think in advance what effect size they would consider to be large enough, and plan the experiment, the hypotheses and the analyses accordingly (see the [experiment and analysis planning FAQ]()).

Nevertheless, more often than not in HCI, it is difficult to determine whether a certain effect is of practical importance. For example, a difference in pointing time of 100 ms between two pointing techniques can be large or small depending on the application, how often it is used, its context of use, etc. In such cases, forcing artificial interpretations of practical importance can hurt transparency. In many cases, it is sufficient to present effect sizes in a clear manner and leave the judgment of practical importance to the reader.

# Exemplars

## How Do I Calculate an Effect Size for A NonParametric Test?
For a nonparametric test that produces a Z-score, like the Mann-Whitney U test or the Wilcoxon Signed-Rank test, an effect size can be computed as:

<center>$r = \left|\frac{Z}{\sqrt{N}}\right|$</center>

Above, Z is the Z-score and N is the number of observations in all groups [@Rosenthal1991a, p. 19). The result, *r*, is a variance-based effect size, like Pearson *r*, not a Cohen *d*-family effect size. The *r* can be squared to estimate the percentage of variance explained, however it will not be exactly equivalent to the Pearson *r*. 

According to [@Cohen1988a], an *r* of 0.1 is “small,” of 0.3 is “medium,” and of 0.5 is “large.” However, one should take care when interpreting effect sizes rotely. Instead, consider the practical differences emerging in one’s study and whether those differences are meaningful in the world.


## Data
Assuming between-subjects design:
```{r}
set.seed(12)
n <- 20
data <- tibble(
  Group = rep(c("A", "B"), each = n),
  TaskCompletionTime_Sec = c(
    rnorm(n, mean = 12, sd = 1),
    rnorm(n, mean = 10, sd = 2)
  )
)
```

## Visualizing data

```{r, fig.height=2, fig.width=3}
pd <- position_dodge(0.2)
data %>% 
  ggplot(aes(x = Group, y = TaskCompletionTime_Sec)) +
  geom_point(alpha = 0.2, position = pd) +
  stat_summary(fun.data = "mean_cl_normal", geom = "pointrange") +
  coord_flip() +
  ylab("Task Completion Time (s)")


```

## Simple effect size

```{r}
library(broom) # for tidy()
data_A <- (data %>% filter(Group == "A"))[[2]]
data_B <- (data %>% filter(Group == "B"))[[2]]
t_result <- tidy(t.test(data_A, data_B))
t_result # result in tabular format
```

*Simple effect size:* The task completion time of the two groups differ by `r t_result$estimate` (95% confidence interval [`r t_result$conf.low`, `r t_result$conf.high`]) seconds.

```{r, fig.height=1, fig.width=3}
t_result %>% 
  ggplot(aes(x = 1, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, color = "red") +
  coord_flip() +
  ylab("Difference in task completion time (s)") +
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

## Standardized effect size

```{r}
library(effsize)
cohen_d <- cohen.d(TaskCompletionTime_Sec ~ Group, data = data)
```

**Standardized effect size:** Cohen's d = `r cohen_d$estimate` SDs with 95% confidence interval [`r cohen_d$conf.int[1]` , `r cohen_d$conf.int[2]`]



## Non-parametric effect size

```{r}
library(coin)
data_A <- (data %>% filter(Group == "A"))[["TaskCompletionTime_Sec"]]
data_B <- (data %>% filter(Group == "B"))[["TaskCompletionTime_Sec"]]
wilcox_result <- wilcox_test(TaskCompletionTime_Sec ~ factor(Group), data = data)
effect_r <- abs(wilcox_result@statistic@teststatistic / sqrt(nrow(data)))
```

**Non-parametric effect size:** Variance-based effect size *r*  = `r effect_r`.


# References

